<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="PacificVis 2018">
  <title>PacificVis 2018</title>

  <!-- Bootstrap core CSS -->
  <link href="/pvis2018/assets/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <link href="/pvis2018/assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <!-- Custom styles for this template -->
  <link href="/pvis2018/assets/css/carousel.css" rel="stylesheet">
  <link href="/pvis2018/assets/css/pvis2018.css" rel="stylesheet">

  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js">
    </script>
  <![endif]-->



</head>

<body>

  <div class="navbar-wrapper">
    <div classs="container">
      <nav class="navbar navbar-inverse navbar-fixed-top">
        <div id="main-navbar" class="container-fluid">
          <div class="navbar-header">
            <button type="button" id="navbar-button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/pvis2018/">IEEE PacificVis 2018</a>
          </div>

          <div id="navbar" class="navbar-collapse collapse" data-toggle="collapse" data-target=".navbar-collapse">
            <ul class="nav navbar-nav">
              <li><a href="#Top">Top</a></li>
              <li class="dropdown"><a href="#reference_to_nowhere" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Social events <span class="caret"></span></a>
                <ul class="dropdown-menu">
                  <li><a href="/pvis2018/program.html#social-events">Social events</a></li>
                  <li><a href="#Reception">Reception</a></li>
                  <li><a href="#Banquet">Banquet</a></li>
                </ul>
              </li>
              <li class="dropdown"><a href="#reference_to_nowhere" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Keynotes <span class="caret"></span></a>
                <ul class="dropdown-menu">
                  <li><a href="/pvis2018/program.html#keynotes">Keynotes</a></li>
                  <li><a href="#Shixia_Liu">Shixia Liu</a></li>
                  <li><a href="#Shuichi_Onami">Shuichi Onami</a></li>
                  <li><a href="#Tim_Dwyer">Tim Dwyer</a></li>
                </ul>
              </li>
              <li class="dropdown"><a href="#reference_to_nowhere" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">PacificVAST <span class="caret"></span></a>
                <ul class="dropdown-menu">
                  <li><a href="#PacificVAST">PacificVAST</a></li>
                  <li><a href="#Session-1-Biomedical-Applications">Session 1 Biomedical Applications</a></li>
                  <li><a href="#Session-2-Text-Analytics">Session 2 Text Analytics</a></li>
                  <li><a href="#Session-3-VA-Design-Tools-and-Systems">Session 3 VA Design Tools and Systems</a></li>
                  <li><a href="#Session-4-VA-Systems-and-Design-Studies">Session 4 VA Systems and Design Studies</a></li>
                </ul>
              </li>
              <li class="dropdown"><a href="#reference_to_nowhere" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">PacificVis <span class="caret"></span></a>
                <ul class="dropdown-menu">
                  <li><a href="#PacificVis">PacificVis</a></li>
                  <li><a href="#Technical_Sessions">Technical Sessions</a></li>
                  <li><a href="#Fast-Forward-and-Lightning-Talk-">Fast Forward and Lightning Talk </a></li>
                  <li><a href="#Interactive-Session-1-">Interactive Session 1 </a></li>
                  <li><a href="#Session-1-Graphs-and-Networks">Session 1 Graphs and Networks</a></li>
                  <li><a href="#Session-2-Volume-Visualization">Session 2 Volume Visualization</a></li>
                  <li><a href="#Session-3-Particles-Molecules-and-Uncertainty">Session 3 Particles Molecules and Uncertainty</a></li>
                  <li><a href="#Interactive-Session-2-">Interactive Session 2 </a></li>
                  <li><a href="#Session-4-Time-Oriented-Data">Session 4 Time Oriented Data</a></li>
                  <li><a href="#Session-5-Visual-Analytics">Session 5 Visual Analytics</a></li>
                  <li><a href="#Session-6-Maps-Text-and-Social-Media">Session 6 Maps Text and Social Media</a></li>
                  <li><a href="#Session-7-Evaluation-and-Immersion">Session 7 Evaluation and Immersion</a></li>
                </ul>
              </li>




              <li><a href="#co-located-event">Co located event</a></li>
            </ul>
          </div>
        </div>
      </nav>
    </div>
  </div>
  <div id="pvis-banner-division">
    <img src="/pvis2018/assets/images/banner/pvis-top-morning.jpg?1258" id="pvis-banner" class="img-fluid" style="width: 100%" alt="Welcome to PacificVis 2018, Kobe!">
    <p id="banner_text">April 10-13, 2018 - Kobe, Japan</p>
  </div>
  <section class="container marketing lead" align="justify" level="1">
    <h1 id="Top"></h1>
    <map name="GraffleExport">
<area shape="rect" coords="1006,1754,1438,2042" href="#Banquet">
<area shape="rect" coords="142,1610,574,1898" href="#social-events">
<area shape="rect" coords="142,1394,574,1513" href="#Session-4-VA-Systems-and-Design-Studies">
<area shape="rect" coords="142,1250,574,1369" href="#Session-3-VA-Design-Tools-and-Systems">
<area shape="rect" coords="142,1106,574,1225" href="#Session-2-Text-Analytics">
<area shape="rect" coords="142,746,574,962" href="#interactive-session-1">
<area shape="rect" coords="142,566,574,746" href="#Session-1-Biomedical-Applications">
<area shape="rect" coords="142,962,574,1106" href="#Shixia_Liu">
<area shape="rect" coords="1006,842,1438,1130" href="#Interactive-Session-2-">
<area shape="rect" coords="574,722,1006,842" href="#Fast-Forward-and-Lightning-Talk-">
<area shape="rect" coords="1006,386,1438,530" href="#Tim_Dwyer">
<area shape="rect" coords="574,842,1006,1130" href="#Interactive-Session-1-">
<area shape="rect" coords="1438,602,1870,842" href="#Session-7-Evaluation-and-Immersion">
<area shape="rect" coords="1438,314,1870,530" href="#Session-6-Maps-Text-and-Social-Media">
<area shape="rect" coords="574,1322,1006,1538" href="#Session-2-Volume-Visualization">
<area shape="rect" coords="1006,1322,1438,1538" href="#Session-5-Visual-Analytics">
<area shape="rect" coords="1006,602,1438,842" href="#Session-3-Particles-Molecules-and-Uncertainty">
<area shape="rect" coords="1006,1130,1438,1249" href="#Session-4-Time-Oriented-Data">
<area shape="rect" coords="574,1130,1006,1249" href="#Session-1-Graphs-and-Networks">
<area shape="rect" coords="574,506,1006,650" href="#Shuichi_Onami">
</map>
    <p><img border="0" src="/pvis2018/assets/images/pvis2018.png?1258" class="img-responsive center-block" usemap="#GraffleExport"></p>
  </section>
  <section class="container marketing lead" align="justify" level="1">
    <h1 id="social-events">Social Events</h1>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Reception">Reception (Apr.&nbsp;10, 18:00)</h2>
      <p>PacificVis 2018 welcomes all the participants with a friendly reception held at a Zoo nearby the conference site!</p>
      <p><a href="https://en.wikipedia.org/wiki/Kobe_Animal_Kingdom">Kobe Animal Kingdom</a> / <a href="https://www.kobe-oukoku.com">神戸どうぶつ王国</a></p>
    </section>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Banquet">Banquet (Apr.&nbsp;12, 19:00)</h2>
      <p>The PacificVis 2018 banquet takes at a SAKE brewery. The brewery is located in Nada, the area that gave birth to SAKE and brought SAKE culture. The brewery produces the finest quality SAKE from traditional method.</p>
      <p><a href="http://enjoyfukuju.com/en">Kobe Shushinkan</a> / <a href="http://www.shushinkan.co.jp">神戸酒心館</a></p>
    </section>
  </section>
  <section class="container marketing lead" align="justify" level="1">
    <h1 id="keynotes">Keynotes</h1>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Shixia_Liu">Prof.&nbsp;Shixia Liu<br>Tsinghua University</h2>
      <p><img class="img-circle" width="200" height="200" src="/pvis2018/assets/images/keynote-liu.jpg?1258" alt="Shixia Liu"></p>
      <dl>
        <dt>Date</dt>
        <dd>April 10 (Tuesday), 13:30
        </dd>
        <dt>Title</dt>
        <dd>Explainable Machine Learning
        </dd>
        <dt>Abstract</dt>
        <dd>Machine learning has demonstrated being highly successful at solving many real-world artificial intelligence and data mining problems ranging from information retrieval. However, most users often treat the machine learning model as a “black box”
          because of its incomprehensible functions and unclear working mechanism. Without a clear understanding of how and why the model works, the development of high-performance models typically relies on a time-consuming trial-and-error procedure.
          This talk presents the major challenges of interactive machine learning and exemplifies the solutions with several visual analytics techniques and examples, including model understanding and diagnosis.
        </dd>
        <dt>Bio</dt>
        <dd>Shixia Liu is an associate professor at Tsinghua University. Her research interests include visual text analytics, visual social analytics, visual behavior analytics, graph visualization, and tree visualization. Before joining Tsinghua University,
          she worked as a lead researcher at Microsoft Research Asia and a research staff member at IBM China Research Lab. Shixia is one of the Papers Co-Chairs of IEEE VAST 2016 and 2017. She is an associate of IEEE Transactions on Visualization and
          Computer Graphics and is on the editorial board of Information Visualization. She was the guest editor of ACM Transactions on Intelligent Systems and Technology and Tsinghua Science and Technology. She was the program co-chair of PacifcVis 2014
          and VINCI 2012. Shixia was in the Steering Committee of VINCI 2013. She is on the organizing committee of IEEE VIS 2015 and 2014. She is/was in the Program Committee for CHI 2018, InfoVis 2015, 2014, VAST 2015, 2014, KDD 2015, 2014, 2013, ACM
          Multimedia 2009, SDM 2008, ACM IUI 2011, 2009, PacificVis 2008, 2009, 2010, 2011, PAKDD 2013, VISAPP 2012, 2011, VINCI 2011.
        </dd>
        <dt>Chair</dt>
        <dd>Issei Fujishiro (Keio University)
        </dd>
      </dl>
    </section>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Shuichi_Onami">Dr.&nbsp;Shuichi Onami<br>RIKEN Quantitative Biology Center</h2>
      <p><img class="img-circle" width="200" height="200" src="/pvis2018/assets/images/keynote-onami.jpg?1258" alt="Shuichi Onami"></p>
      <dl>
        <dt>Date</dt>
        <dd>April 11 (Wednesday), 10:20
        </dd>
        <dt>Title</dt>
        <dd>Visualization accelerates data-driven developmental biology
        </dd>
        <dt>Abstract</dt>
        <dd>Recent advancements in live imaging technologies and those in bioimage informatics technologies have enabled quantitative measurement of spatiotemporal dynamics of biosystems. The resultant accumulation of quantitative data of various biosystems
          dynamics, combined with single-cell resolution and/or whole-genome level gene expression data now provides new opportunities in biology, i.e., data-driven biology. In this talk, I will overview this new direction in biology by showing our on-going
          researches on animal development. I will discuss novel computational method to deduce quantitative model of animal development by using a large collection of quantitative data of nuclear dynamics in animal development. I will also discuss how
          visualization can contribute to data-driven biology by showing our collaborative works with scientists in visualization field.
        </dd>
        <dt>Bio</dt>
        <dd>Shuichi Onami received his D.V.M. from The University of Tokyo in 1994, and received his Ph.D.&nbsp;from Department of the Genetics, The Graduate University for Advanced Studies in 1998. He worked as a researcher and then group leader in ERATO
          Kitano Symbiotic Systems Project at Japan Science and Technology Corporation (1999–2001), and became independent as an associate professor at Keio University (2002–2006). He joined RIKEN Genomic Sciences Center as a senior scientist in 2006,
          and became a team leader at RIKEN Advanced Science Institute in 2008. Since 2011, he is a team leader at RIKEN Quantitative Biology Center. He invented an automated system for measuring cell division dynamics in C. elegans embryos and pioneered
          quantitative modeling in C. elegans embryo. His current research interests include mathematical modeling of animal development, knowledge extraction from a large-scale high-dimensional biological data, and their applications to human biology.
        </dd>
        <dt>Chair</dt>
        <dd>Shuichi Onami (RIKEN Quantitative Biology Center)
        </dd>
      </dl>
    </section>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Tim_Dwyer">Prof.&nbsp;Tim Dwyer<br>Monash University</h2>
      <p><img class="img-circle" width="200" height="200" src="/pvis2018/assets/images/keynote-dwyer.jpg?1258" alt="Tim Dwyer"></p>
      <dl>
        <dt>Date</dt>
        <dd>April 12 (Thursday), 9:30 am
        </dd>
        <dt>Title</dt>
        <dd>Immersive analytics: Interactive data analysis using the surfaces and spaces around us
        </dd>
        <dt>Abstract</dt>
        <dd>The goal of immersive analytics is to remove barriers between people, their data, and the tools they use for analysis. It aims to support data understanding and decision making everywhere and by everyone; working individually or collaboratively
          in collocated or distributed groups. It aims to make tools that are intuitive, engaging and make the best possible use of all sensory channels. While this may be achieved through the use of immersive virtual environment technologies, data physicalisation,
          natural interfaces or responsive analytics, the field of immersive analytics is not restricted to the use of such technologies. Immersive Analytics was initiated as a field of research at Monash University in 2014. In this talk I will discuss
          ongoing Immersive Analytics research at Monash University and elsewhere in this now global initiative.
        </dd>
        <dt>Bio</dt>
        <dd>Tim Dwyer received his PhD on “Two and a Half Dimensional Visualisation of Relational Networks” from the University of Sydney in 2005. He was a post-doctoral Research Fellow at Monash University from 2005 to 2008, then a Visiting Researcher at
          Microsoft Research, USA in 2008-2009. From 2009 to 2012 he worked as a Senior Software Development Engineer with the Visual Studio product group at Microsoft, USA. A highlight of this period was shipping the Code Map software dependency visualisation
          tool with Visual Studio 2012. In late 2012 he returned to Monash University as a Larkins Fellow where he now co-directs the Immersive Analytics Initiative and is a founding member of the Monash Adaptive Visualisation Lab.
        </dd>
        <dt>Chair</dt>
        <dd>Shigeo Takahashi (University of Aizu)
        </dd>
      </dl>
    </section>
  </section>
  <section class="container marketing lead" align="justify" level="1">
    <h1 id="PacificVAST">PacificVAST Program</h1>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Session-1-Biomedical-Applications">Session 1: Biomedical Applications</h2>
      <p>
        <session-start>April 10 (Tuesday) 10:45</session-start>
        <chair>Chair: Wei Chen</chair> (
        <affiliation>Zhejiang University</affiliation>)</p>
      <p>
        <talk>
          <talk-title>A Visual Analytics System to Support the Formation of a Hypothesis from Calcium Waves Data</talk-title>
          <author>
            <author-name>Kozen Umezawa</author-name> (
            <affiliation>Kyoto University, Japan</affiliation>)</author>
          <author>
            <author-name>Hiroaki Natsukawa</author-name> (
            <affiliation>Kyoto University, Japan</affiliation>)</author>
          <author>
            <author-name>Yosuke Onoue</author-name> (
            <affiliation>Kyoto University, Japan</affiliation>)</author>
          <author>
            <author-name>Koji Koyamada</author-name> (
            <affiliation>Kyoto University, Japan</affiliation>)</author>
          <talk-abstract> In most species, calcium waves in the oocyte are considered common phenomena in the activation of eggs. However, the mechanism of calcium waves has not yet been clarified. By collaborating with biologists studying Caenorhabditis elegans (C.
            elegans), which is widely used as a model organism, we observed that the following requirements must be satisfied to form a useful hypothesis based on calcium waves captured using high-speed in vivo imaging: (1) the ability to obtain an overview
            of how the calcium waves are propagated and (2) the ability to understand the propagation of waves in a narrow region. However, conventional visualization methods cannot satisfy these requirements simultaneously. Therefore, we propose a visual
            analytics system that allows users to understand and explore calcium wave images using cross-correlation analysis of the time-series data of the Ca2+ fluorescence intensity at each point. The interface of this system comprises an overview
            visualization, a detail visualization, and user interactions to satisfy these requirements and realize exploratory visualization. Some views present an overview visualization that displays the clustering results of a directed graph calculated
            using cross-correlation analysis. These views enable the users to understand the overview of wave propagation, thereby helping users find a region of interest. The detail visualization shows the relationship between the region of interest
            and other areas. Furthermore, users can use the proposed system with overview-detail and brush-link exploration to assign meaning to the region of interest and construct a hypothesis for its role. In this paper, we demonstrate how the proposed
            visual analytics approach works and how new hypotheses can be formed using the analysis of C. elegans calcium waves. </talk-abstract>
        </talk>
      </p>
      <p>
        <talk>
          <talk-title>MultiSciView: Multivariate Scientific X-ray Image Visual Exploration with Cross-Data Space Views</talk-title>
          <author>
            <author-name>Wen Zhong</author-name> (
            <affiliation>Stony Brook University, USA</affiliation>)</author>
          <author>
            <author-name>Wei Xu</author-name> (
            <affiliation>Brookhaven National Lab, USA</affiliation>)</author>
          <author>
            <author-name>Kevin G. Yager</author-name> (
            <affiliation>Brookhaven National Laboratory, USA</affiliation>)</author>
          <author>
            <author-name>Gregory S. Doerk</author-name> (
            <affiliation>Brookhaven National Laboratory, USA</affiliation>)</author>
          <author>
            <author-name>Jian Zhao</author-name> (
            <affiliation>FX Palo Alto Laboratory, USA</affiliation>)</author>
          <author>
            <author-name>Yunke Tian</author-name> (
            <affiliation>Midea Emerging Technology Center, USA</affiliation>)</author>
          <author>
            <author-name>Sungsoo Ha</author-name> (
            <affiliation>Brookhaven National Laboratory, USA</affiliation>)</author>
          <author>
            <author-name>Cong Xie</author-name> (
            <affiliation>Stony Brook University, USA</affiliation>)</author>
          <author>
            <author-name>Yuan Zhong</author-name> (
            <affiliation>Facebook, USA</affiliation>)</author>
          <author>
            <author-name>Klaus Mueller</author-name> (
            <affiliation>Stony Brook University, USA</affiliation>)</author>
          <author>
            <author-name>Kerstin Kleese Van Dam</author-name> (
            <affiliation>Brookhaven National Laboratory, USA</affiliation>)</author>
          <talk-abstract> X-ray images obtained from synchrotron beamlines are large-scale, high-resolution and high-dynamic-range grayscale data encoding multiple complex properties of the measured materials. They are typically associated with a variety of metadata
            which increases their inherent complexity. There is a wealth of information embedded in these data but so far scientists lack modern exploration tools to unlock these hidden treasures. To bridge this gap, we propose MultiSciView, a multivariate
            scientific x-ray image visualization and exploration system for beamline-generated x-ray scattering data. Our system is composed of three complementary and coordinated interactive visualizations to enable a coordinated exploration across the
            images and their associated attribute and feature spaces. The first visualization features a multi-level scatterplot visualization dedicated for image exploration in attribute, image, and pixel scales. The second visualization is a histogram-based
            attribute cross filter by which users can extract desired subset patterns from data. The third one is an attribute projection visualization designed for capturing global attribute correlations. We demonstrate our framework by ways of a case
            study involving a real-world material scattering dataset. We show that our system can efficiently explore large-scale x-ray images, accurately identify preferred image patterns, anomalous images and erroneous experimental settings, and effectively
            advance the comprehension of material nanostructure properties. </talk-abstract>
        </talk>
      </p>
      <p>
        <talk>
          <talk-title>An Uncertainty-aware Workflow for Keyhole Surgery Planning using Hierarchical Image Semantics</talk-title>
          <author>
            <author-name>Christina Gillmann</author-name> (
            <affiliation>TU Kaiserslautern, Germany</affiliation>)</author>
          <author>
            <author-name>Robin Georg Claus Maack</author-name> (
            <affiliation>University of Kaiserslautern, Germany</affiliation>)</author>
          <author>
            <author-name>Tobias Post</author-name> (
            <affiliation>Technical University of Kaiserslautern, Germany</affiliation>)</author>
          <author>
            <author-name>Thomas Wischgoll</author-name> (
            <affiliation>Wright State University, USA</affiliation>)</author>
          <author>
            <author-name>Hans Hagen</author-name> (
            <affiliation>University of Kaiserslautern, Germany</affiliation>)</author>
          <talk-abstract> Keyhole surgeries become increasingly important in clinical daily routine as they help minimizing the damage of a patient’s healthy tissue. The planning of keyhole surgeries is based on medical imaging and an important factor that influences
            the surgeries’ success. Due to the image reconstruction process, medical image data contains uncertainty that exacerbates the planning of a keyhole surgery. In this paper we present a visual workflow that helps clinicians to examine and compare
            different surgery paths as well as visualizing the patients’ affected tissue. The analysis is based on the concept of hierarchical image semantics, that segment the underlying image data with respect to the input images’ uncertainty and the
            users understanding of tissue composition. Users can define arbitrary surgery paths that they need to investigate further. The defined paths can be queried by a rating function to identify paths that fulfill user-defined properties. The workflow
            allows a visual inspection of the affected tissues and its substructures. Therefore, the workflow includes a linked view system indicating the three-dimensional location of selected surgery paths as well as how these paths affect the patients
            tissue. To show the effectiveness of the presented approach, we applied it to the planning of a keyhole surgery of a brain tumor removal and a kneecap surgery. </talk-abstract>
        </talk>
      </p>
    </section>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Session-2-Text-Analytics">Session 2: Text Analytics</h2>
      <p>
        <session-start>April 10 (Tuesday) 14:30</session-start>
        <chair>Chair: Yun Jang</chair> (
        <affiliation>Sejong University</affiliation>)</p>
      <p>
        <talk>
          <talk-title>MessageLens: A Visual Analytics System to Support Multifaceted Exploration of MOOC Forum Discussions</talk-title>
          <author>
            <author-name>Jian-Syuan Wong</author-name> (
            <affiliation>Pennsylvania State University, USA</affiliation>)</author>
          <author>
            <author-name>Xiaolong Luke Zhang</author-name> (
            <affiliation>Pennsylvania State University, USA</affiliation>)</author>
          <talk-abstract> Massive Open Online Courses (MOOCs) often provide online discussion forum tools to facilitate learner interaction and communication. Having massive forum messages posted by learners everyday, MOOC forums are regarded as an important source for
            understanding learners activities and opinions. However, the high volume and heterogeneity of MOOC forum contents make it challenging to analyze forum data eectively from dierent perspectives of discussions and to integrate diverse information
            into a coherent understanding of issues of concern. In this paper, we report a study on the design of a visual analytics tool to facilitate the multifaceted analysis of online discussion forums. This tool, called MessageLens, aims at helping
            MOOC instructors to gain a better understanding of forum discussions from three facets: discussion topic, learner attitude, and communication among learners. With various visualization tools, instructors can investigate learner activities
            from dierent perspectives. We report a case study with real-world MOOC forum data to present the features of MessageLens and a preliminary evaluation study on the benefits and areas of improvement of the system . Our research suggests an
            approach to analyzing rich communication contents as well as dynamic social interactions among people. </talk-abstract>
        </talk>
      </p>
      <p>
        <talk>
          <talk-title>Metro-Wordle: An Interactive Visualization for Urban Text Distributions based on Wordle</talk-title>
          <author>
            <author-name>Chenlu Li</author-name> (
            <affiliation>Shanghai Jiao Tong University, China</affiliation>)</author>
          <author>
            <author-name>Xiaoju Dong</author-name> (
            <affiliation>Shanghai Jiao Tong University, China</affiliation>)</author>
          <author>
            <author-name>Xiaoru Yuan</author-name> (
            <affiliation>Peking University, China</affiliation>)</author>
          <talk-abstract> With the development of cities and the explosion of information, vast amounts of geo-tagged textural data about Points of Interests (POIs) have been generated. Extracting useful information and discovering text spatial distributions from the
            data are challenging and meaningful. Also, the huge numbers of POIs in modern cities make it important to have efficient approaches to retrieve and choose a destination. This paper provides a visual design combing metro map and wordles to
            meet the needs. In this visualization, metro lines serve as the divider lines splitting the city into several subareas and the boundaries to constrain wordles within each subarea. The wordles are generated from keywords extracted from the
            text about POIs (including reviews, descriptions, etc.) and embedded into the subareas based on their geographical locations. By generating intuitive results and providing an interactive visualization to support exploring text distribution
            patterns, our strategy can guide the users to explore urban spatial characteristics and retrieve a location efficiently. Finally, we implement a visual analysis of the restaurants data in Shanghai, China as a case study to evaluate our strategy.
          </talk-abstract>
        </talk>
      </p>
    </section>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Session-3-VA-Design-Tools-and-Systems">Session 3: VA Design Tools and Systems</h2>
      <p>
        <session-start>April 10 (Tuesday) 15:30</session-start>
        <chair>Chair: Xiaoru Yuan</chair> (
        <affiliation>Peking University</affiliation>)</p>
      <p>
        <talk>
          <talk-title>TideGrapher: Visual Analytics of Tactical Situations for Rugby Matches</talk-title>
          <author>
            <author-name>Yusuke Ishikwa</author-name> (
            <affiliation>Keio University, Japan</affiliation>)</author>
          <author>
            <author-name>Issei Fujishiro</author-name> (
            <affiliation>Keio University, Japan</affiliation>)</author>
          <talk-abstract> Various attempts at exploiting information visualization for sports have recently been reported in the literature, although it is still challenging to analyze continuous ball matches. In this paper, we propose a novel visual analytics system,
            called TideGrapher, to track the transition of tactile situations in a rugby match. With a particular focus on the side position of the ball, we designed a dedicated spatial substrate based on the spatio-temporal trajectory of the ball and
            provided a set of basic interactions. Quantitative analysis was strengthened by adding a new index, called initiative, to commonly used possession (ball occupation) and territory (dominance of territory). The feasibility of the proposed visual
            analytics system was proven empirically through application to datasets from real amateur and professional matches. </talk-abstract>
        </talk>
      </p>
      <p>
        <talk>
          <talk-title>VisComposer: A Visual Programmable Composition Environment for Information Visualization </talk-title>
          <author>
            <author-name>Honghui Mei</author-name> (
            <affiliation>Zhejiang University, China</affiliation>)</author>
          <author>
            <author-name>Wei Chen</author-name> (
            <affiliation>Zhejiang University, China</affiliation>)</author>
          <author>
            <author-name>Yuxin Ma</author-name> (
            <affiliation>Zhejiang University, China</affiliation>)</author>
          <author>
            <author-name>Huihua Guan</author-name> (
            <affiliation>Zhejiang University, China</affiliation>)</author>
          <author>
            <author-name>Wanqi Hu</author-name> (
            <affiliation>Zhejiang University, China</affiliation>)</author>
          <talk-abstract> As the amount of data being collected has increased, the need for tools that can enable the visual exploration of data has also grown. This has led to the development of a variety of widely used programming frameworks for information visualization.
            Unfortunately, such frameworks demand comprehensive visualization and coding skills and require users to develop visualization from scratch. An alternative is to create interactive visualization design environments that require little to no
            programming. However, these tools only supports a small portion of visual forms. We present a programmable integrated development environment (IDE), VisComposer, that supports the development of expressive visualization using a drag-and-drop
            visual interface. VisComposer exposes the programmability by customizing desired components within a modularized visualization composition pipeline, effectively balancing the capability gap between expert coders and visualization artists.
            The implemented system empowers users to compose comprehensive visualizations with real-time preview and optimization features, and supports prototyping, sharing and reuse of the effects by means of an intuitive visual composer. Visual programming
            and textual programming integrated in our system allow users to compose more complex visual effects while retaining the simplicity of use. We demonstrate the performance of VisComposer with a variety of examples and an informal user evaluation.
          </talk-abstract>
        </talk>
      </p>
    </section>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Session-4-VA-Systems-and-Design-Studies">Session 4: VA Systems and Design Studies</h2>
      <p>
        <session-start>April 10 (Tuesday) 16:30</session-start>
        <chair>Chair: Jiawan Zhang</chair> (
        <affiliation>Tianjin University</affiliation>)</p>
      <p>
        <talk>
          <talk-title>LongLine: Visual Analytics System for Large-scale Audit Logs</talk-title>
          <author>
            <author-name>Seunghoon Yoo</author-name> (
            <affiliation>Seoul National University, Republic of Korea</affiliation>)</author>
          <author>
            <author-name>Jaemin Jo</author-name> (
            <affiliation>Seoul National University, Republic of Korea</affiliation>)</author>
          <author>
            <author-name>Bohyoung Kim</author-name> (
            <affiliation>Hankuk University of Foreign Studies, Republic of Korea</affiliation>)</author>
          <author>
            <author-name>Jinwook Seo</author-name> (
            <affiliation>Seoul National University, Republic of Korea</affiliation>)</author>
          <talk-abstract> Audit logs are different from other software logs in that they record the most primitive events (i.e., system calls) in modern operating systems. Audit logs contain a detailed trace of an operating system, and thus have received great attention
            from security experts and system administrators. However, the complexity and size of audit logs, which increase in real time, have hindered analysts from understanding and analyzing them. In this paper, we present a novel visual analytics
            system, LongLine, which enables interactive visual analyses of large-scale audit logs. LongLine lowers the interpretation barrier of audit logs by employing human-understandable representations (e.g., file paths and commands) instead of abstract
            indicators of operating systems (e.g., file descriptors) as well as revealing the temporal patterns of the logs in a multi-scale fashion with meaningful granularity of time in mind (e.g., hourly, daily, and weekly). LongLine also streamlines
            comparative analysis between interesting subsets of logs, which is essential in detecting anomalous behaviors of systems. In addition, LongLine allows analysts to monitor the system state in a streaming fashion, keeping the latency between
            log creation and visualization less than one minute. Finally, we evaluate our system through a case study and a scenario analysis with security experts. </talk-abstract>
        </talk>
      </p>
      <p>
        <talk>
          <talk-title>A Visual Analytics System for Optimizing the Performance of Large-Scale Networks in Supercomputing Systems</talk-title>
          <author>
            <author-name>Takanori Fujiwara</author-name> (
            <affiliation>University of California Davis, USA</affiliation>)</author>
          <author>
            <author-name>Jianping Li</author-name> (
            <affiliation>University of California Davis, USA</affiliation>)</author>
          <author>
            <author-name>Misbah Mubarak</author-name> (
            <affiliation>Argonne National Laboratory, USA</affiliation>)</author>
          <author>
            <author-name>Caitlin Ross</author-name> (
            <affiliation>Rensselaer Polytechnic Institute, USA</affiliation>)</author>
          <author>
            <author-name>Christopher Carothers</author-name> (
            <affiliation>Rensselaer Polytechnic Institute, USA</affiliation>)</author>
          <author>
            <author-name>Robert Ross</author-name> (
            <affiliation>Argonne National Laboratory, USA</affiliation>)</author>
          <author>
            <author-name>Kwan-Liu Ma</author-name> (
            <affiliation>University of California Davis, USA</affiliation>)</author>
          <talk-abstract> The overall efficiency of an extreme-scale supercomputer largely relies on the performance of its network interconnects. Several of the state of the art supercomputers use networks based on the increasingly popular Dragonfly topology. It is
            crucial to study the behavior and performance of different parallel applications running on Dragonfly networks in order to make optimal system configurations and design choices, such as job scheduling and routing strategies. However, in order
            to study these temporal network behavior, we would need a tool to analyze and correlate numerous sets of multivariate time-series data collected from the Dragonfly’s multi-level hierarchies. This paper presents such a tool–a visual analytics
            system–that uses the Dragonfly network to investigate the temporal behavior and optimize the communication performance of a supercomputer. We coupled interactive visualization with time-series analysis methods to help reveal hidden patterns
            in the network behavior with respect to different parallel applications and system configurations. Our system also provides multiple coordinated views for connecting behaviors observed at different levels of the network hierarchies, which
            effectively helps visual analysis tasks. We demonstrate the effectiveness of the system with a set of case studies. Our system and findings can not only help improve the communication performance of supercomputing applications, but also the
            network performance of next-generation supercomputers. </talk-abstract>
        </talk>
      </p>
    </section>
  </section>
  <section class="container marketing lead" align="justify" level="1">
    <h1 id="PacificVis">PacificVis Program</h1>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Technical_Sessions">Technical Sessions</h2>
      <ul>
        <li>
          <p><a href="#Fast-Forward-and-Lightning-Talk">Fast Forward and Lightning Talk</a></p>
        </li>
        <li>
          <p><a href="#Interactive-Session-1">Interactive Session 1</a></p>
        </li>
        <li>
          <p><a href="#session-1-graphs-and-networks">Graphs and Networks</a>: Graphs and Networks</p>
        </li>
        <li>
          <p><a href="#session-2-volume-visualization">Volume Visualization</a>: Volume Visualization</p>
        </li>
        <li>
          <p><a href="#session-3-particles-molecules-and-uncertainty">Particles, Molecules, and Uncertainty</a>: Particles, Molecules, and Uncertainty</p>
        </li>
        <li>
          <p><a href="#Interactive-Session-2">Interactive Session 2</a></p>
        </li>
        <li>
          <p><a href="#session-4-time-oriented-data">Time-Oriented Data</a>: Time-Oriented Data</p>
        </li>
        <li>
          <p><a href="#session-5-visual-analytics">Visual Analytics</a>: Visual Analytics</p>
        </li>
        <li>
          <p><a href="#session-6-maps-text-and-social-media">Maps, Text, and Social Media</a>: Maps, Text, and Social Media</p>
        </li>
        <li>
          <p><a href="#session-7-evaluation-and-immersion">Evaluation and Immersion</a>: Evaluation and Immersion</p>
        </li>
      </ul>
    </section>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Fast-Forward-and-Lightning-Talk-">Fast Forward and Lightning Talk:</h2>
      <p>
        <session-start>April 11 (Wednesday) 11:50</session-start>
        <session-chair>Chair: Masahiko Itoh (
          <affiliation>The University of Tokyo</affiliation>), Hidenori Watanave (
          <affiliation>Tokyo Metropolitan University</affiliation>)</session-chair>
      </p>
      <ul>
        <li>
          <p><a href="#interactive-session-1">A list of poster presentations (Interactive Session 1)</a></p>
        </li>
        <li>
          <p><a href="#interactive-session-2">A list of poster presentations (Interactive Session 2)</a></p>
        </li>
        <li>
          <p><a href="#visual-data-storytelling-contest-1">A list of visual data storytelling contest presentations #1</a></p>
        </li>
        <li>
          <p><a href="#visual-data-storytelling-contest-2">A list of visual data storytelling contest presentations #2</a></p>
        </li>
      </ul>
    </section>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Interactive-Session-1-">Interactive Session 1:</h2>
      <p>
        <session-start>April 11 (Wednesday) 12:40</session-start>
        <talk>
          <talk-title>Marionette UI: Guiding Human Body Motion in Real-time for Skill Acquisition</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Kazuki Osamura</author-name> (
            <affiliation>Fujitsu Laboratories Ltd., Japan</affiliation>)</author>
          <talk-abstract>We propose an AR-based system named “Marionette User Interface” that can induce awareness of human body motion skills. This system can detect the difference in human body motion between experts and users. Then, it can visualize the degree of
            the difference with a straight line. In addition, it gives a visual feedback of the future results caused by the difference. We think that users will perceive their own motion skills while looping this feedback. We conducted a user study in
            order to verify the effectiveness of this system. The results show that our system can give awareness of the skills more clearly than the previous system.</talk-abstract>
        </talk>
        <talk>
          <talk-title>The Structure-aware Viewpoint Selection for 3D Branching Structures</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Jaehyun Jang</author-name> (
            <affiliation>KAIST, Korea, Republic of</affiliation>)</author>
          <author>
            <author-name>Jinah Park</author-name> (
            <affiliation>KAIST, Korea, Republic of</affiliation>)</author>
          <talk-abstract>Viewpoint selection is a fundamental technique in computer-aided external guidance. In this work, we present a structure-aware viewpoint selection technique for exploring a tube-like object to reveal the possible paths in complex branching area.
            We define a transition region and adjacent segments from the branch of interests (BOI) from the branch structure in order to abstract structure information for computing context-based viewpoint entropy. We sample some positions on the bounding
            sphere, with respect to BOI, to capture the average information of the branching area, and we compute self-occlusion effects at each position. We further propose the effectiveness of structure information (ESI) for automatic viewpoint selection
            based on our BOI construction and sampled positions. We examine the proposed measure of the context-based viewpoint entropy on CTA medical image data of a 3D cerebrovascular structure as well as leg artery structure. Results show that the
            highest score corresponds to the best view which delivers the structure information visually about the branching area and that the view automatically chosen by fast view selection falls within the reasonably high-valued area.</talk-abstract>
        </talk>
        <talk>
          <talk-title>A Linked Visual Interface of Citation and Co-author Relationships for Surveying Research Papers</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Rina Nakazawa</author-name> (
            <affiliation>Ochanomizu University, Japan</affiliation>)</author>
          <author>
            <author-name>Takayuki Itoh</author-name> (
            <affiliation>Ochanomizu University, Japan</affiliation>)</author>
          <author>
            <author-name>Takafumi Saito</author-name> (
            <affiliation>Tokyo University of Agriculture &amp; Tech., Japan</affiliation>)</author>
          <talk-abstract>When we survey research papers, we usually use a text-based search engine. For novice researchers, a text-based search is sometimes difficult because they do not know appropriate keywords or do not understand the positions of papers. Many visualization
            tools of citation networks have been proposed to help this task. We propose a linked visualization tool of a citation network and a co-author network for surveying research papers. Our technique visualizes both citation and co-author networks
            at the same time. It treats both a paper and an author as bags of words, clustering them applying LDA (Latent Dirichlet Allocation) at the same time. Based on the clustering result, it places the clusters of a citation network by a hybrid
            force-directed and space-filling algorithm. Our technique reuses the cluster positions of a citation network as the initial cluster positions of a co-author network, supposing there are a large number of authors and the set of topics representing
            the authors can be same as the set of the citation network.</talk-abstract>
        </talk>
        <talk>
          <talk-title>Spatio-Temporal Visualization of Tweet Data Using VR</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Kaya Okada</author-name> (
            <affiliation>Ochanomizu University, Japan</affiliation>)</author>
          <author>
            <author-name>Mitsuo Yoshida</author-name> (
            <affiliation>Toyohashi University of Technology, Japan</affiliation>)</author>
          <author>
            <author-name>Takayuki Itoh</author-name> (
            <affiliation>Ochanomizu University, Japan</affiliation>)</author>
          <author>
            <author-name>Tobias Czauderna</author-name> (
            <affiliation>Monash University, Australia</affiliation>)</author>
          <author>
            <author-name>Kingsley Stephens</author-name> (
            <affiliation>Monash University, Australia</affiliation>)</author>
          <talk-abstract>Social media analysis is important to understand people behavior. Human behavior in social media is often related to time and location, which is often difficult to understand the characteristics appropriately and quickly. We chose to visualize
            the data applying a virtual reality technology, which makes us easier to explore the data interactively and intuitively. This poster presents our visualization with tweets of microblogs with location information. Our system includes a three-dimensional
            temporal visualization which consists of the two-dimensional map and a time axis. In particular, we aggregate the number of tweets of each coordinate, calculate scores and display them as piled cubes. We highlight only specific cubes so that
            users can understand the overall tendency of datasets. We also developed user interfaces for operating these cubes and panels which indicate details of tweets.</talk-abstract>
        </talk>
        <talk>
          <talk-title>A Fully Parallel Particle-based Volume Rendering for Large-Scale Unstructured Volume Datasets</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Kengo Hayashi</author-name> (
            <affiliation>Graduate School of System Informatics Kobe University, Japan</affiliation>)</author>
          <author>
            <author-name>Yoshiaki Yamaoka</author-name> (
            <affiliation>Kobe University, Japan</affiliation>)</author>
          <author>
            <author-name>Naohisa Sakamoto</author-name> (
            <affiliation>Kobe University, Japan</affiliation>)</author>
          <author>
            <author-name>Jorji Nonaka</author-name> (
            <affiliation>RIKEN AICS, Japan</affiliation>)</author>
          <talk-abstract>Large-scale simulation results from high performance computing (HPC) systems have continuously increased in size and complexity, thus making the visualization and exploration tasks more challenging. Although volume rendering is widely recognized
            as an effective visualization technique, its usage on large-scale distributed unstructured volume datasets can become a difficult task because of the necessary and time-consuming visibility sorting process. In this work, we focused on the
            particle-based volume rendering (PBVR) method, because of its visibility sorting-free nature, for handling such datasets. We propose a fully parallel PBVR by parallelizing also the particle rendering phase, which remained serial (non-parallel)
            in the previous proposed approaches. To verify the effectiveness of the proposed approach, we evaluated by using a large-scale distributed unstructured thermal fluid simulation result, and could obtain encouraging results for pushing forward
            to continue further developments and improvements in this approach.</talk-abstract>
        </talk>
        <talk>
          <talk-title>Visualizing Dynamic Network via Sampled Massive Sequence View</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Yanmin She</author-name> (
            <affiliation>Central South University, China</affiliation>)</author>
          <author>
            <author-name>Wenjiang Chen</author-name> (
            <affiliation>Central South University, China</affiliation>)</author>
          <author>
            <author-name>Feng Luo</author-name> (
            <affiliation>Central South University, China</affiliation>)</author>
          <author>
            <author-name>JunRong Liu</author-name> (
            <affiliation>Institute of Information Engineering, Chinese Academy of Science, China</affiliation>)</author>
          <author>
            <author-name>Fangfang Zhou</author-name> (
            <affiliation>Central South University, China</affiliation>)</author>
          <author>
            <author-name>Ying Zhao</author-name> (
            <affiliation>Central south university, China</affiliation>)</author>
          <talk-abstract>Massive Sequence View is an important timeline-based technique for dynamic network visualization. However, MSV often suffers from severe visual clutter when limited screen space holds excessive network edges. In this paper, inspired by the use
            of graph sampling in static graph analysis, we propose to utilize graph sampling to reduce visual clutter in MSV. An edge sampling method based on accept-reject random sampling is designed for visualizing dynamic network via MSV. The method
            is able to improve the overall readability of MSV while preserving time-varying network behaviors. This method is also a preliminary attempt to apply graph sampling technique into dynamic network analysis.</talk-abstract>
        </talk>
        <talk>
          <talk-title>Events-based visual analytics for team-based invasion sports</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Kun Zhao</author-name> (
            <affiliation>IBM Research Tokyo, Japan</affiliation>)</author>
          <author>
            <author-name>Takayuki Osogami</author-name> (
            <affiliation>IBM Research Tokyo, Japan</affiliation>)</author>
          <author>
            <author-name>Tetsuro Morimura</author-name> (
            <affiliation>IBM Research Tokyo, Japan</affiliation>)</author>
          <talk-abstract>Traditional techniques for evaluating team performance in team-based invasion sports (e.g.&nbsp;soccer, basketball etc.) rely on the trajectories (sequences of locations) of all players. However, such trajectory data is huge and noisy, making
            the analysis difficult. In this paper, we focus our research on events occurred in sports and analyze the match by evaluating the value of those events. We propose a new method to extract significant events based on statistical features and
            evaluate the event value based on reinforcement learning. The evaluated value is visually inspected to understand how much each event can potentially contribute to scores. The experimental results with real soccer data show the effectiveness
            of the proposed approach. </talk-abstract>
        </talk>
        <talk>
          <talk-title>Comparison of Visual Impression Given by Texture of Real Surfaces and Synthesized Images</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Atsushi Takemoto</author-name> (
            <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
          <author>
            <author-name>Taishi Fujiwara</author-name> (
            <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
          <author>
            <author-name>Kensuke Tobitani</author-name> (
            <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
          <author>
            <author-name>Yusuke Tani</author-name> (
            <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
          <author>
            <author-name>Noriko Nagata</author-name> (
            <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
          <talk-abstract>We present a method to compare visual impressions given by the textures of real surfaces and 3D synthesized images of synthetic resins to improve the reality of CG images. We analyzed the structures of the visual impressions by conducting subjective
            evaluation experiments using real textures and generated images of the textures based on measurements of surface characteristics. We clarified the relationship between the visual impressions of the textures and the surface characteristics
            and showed the possibility of feedback to improve the reality of CG images.</talk-abstract>
        </talk>
        <talk>
          <talk-title>Visual Analysis of Urban Public Services for Early Warning</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Jie Lin</author-name> (
            <affiliation>Tianjin University, China</affiliation>)</author>
          <author>
            <author-name>Ye Yuan</author-name> (
            <affiliation>Tianjin University, China</affiliation>)</author>
          <author>
            <author-name>Chongke Bi</author-name> (
            <affiliation>Tianjin University, China</affiliation>)</author>
          <talk-abstract>Urban public service data is very difficult for urban managers to be understood. The data include spatio-temporal dimensions, and a variety of industries. In order to help city managers understand such kind of multiple and complex data intuitively,
            we designed an interactive visual analysis scheme for the early warning of urban public service. Firstly, an interactive visualization scheme is designed to visualize the data that managers care about, enabling city managers to interactive
            and clearly observe the problems of a city’s public services. Then, these data are analyzed by a new visual analysis scheme to predict the possible threats and provide early warning for the urban public services (such as the explosion caused
            by gas pipeline aging). Our interactive visual analysis scheme can help city managers to improve the city’s livability.</talk-abstract>
        </talk>
        <talk>
          <talk-title>Visualization of Eye Tracking Data Using a Directed Graph with Edge Bundling</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Yuri Miyagi</author-name> (
            <affiliation>Ochanomizu University, Japan</affiliation>)</author>
          <author>
            <author-name>Daniel Weiskopf</author-name> (
            <affiliation>University of Stuttgart, Germany</affiliation>)</author>
          <author>
            <author-name>Takayuki Itoh</author-name> (
            <affiliation>Ochanomizu University, Japan</affiliation>)</author>
          <talk-abstract>Analysis of eye tracking data is useful to understand human behavior and mentality. For example, if many customers looked a particular product, we can suppose the product is attractive. Visualization facilitates efficient analysis of eye tracking
            data. Especially, drawing specific trajectories of eye tracking scan-paths makes it easy to understand such data. However, simply drawing original paths often results in complicated pictures. People often look at objects repetitiously, which
            means that the scan-path has many crossings. In this poster, we propose a technique to visualize eye tracking data as directed graphs. Currently, we focus on static stimuli that have a clear structure of the area of interests, such as posters
            and websites. We suppose that users divide the stimulus into multiple regions to define the areas of interest at first. The system generates a directed graph from the region division result and eye tracking data. Furthermore, the system can
            apply edge bundling. The bundling arranges the graph in a simplified visual representation by reducing crossings of edges. As a use case, we visualize eye tracking data from a participant checking a website. The result shows that the system
            can visualize features of eye tracking scan-paths in combination with a picture of the stimulus.</talk-abstract>
        </talk>
        <talk>
          <talk-title>Predictive Analytics for Youth Physical Growth Data</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Hanbyul Yeon</author-name> (
            <affiliation>Sejong University, Korea</affiliation>)</author>
          <author>
            <author-name>Mingyu Pi</author-name> (
            <affiliation>Sejong university, Korea, Republic of</affiliation>)</author>
          <author>
            <author-name>Seongbum Seo</author-name> (
            <affiliation>Sejong University, Republic of Korea</affiliation>)</author>
          <author>
            <author-name>Yun Jang</author-name> (
            <affiliation>Sejong University, South Korea</affiliation>)</author>
          <talk-abstract>Physical growth data consists of small discrete pieces of time-series data containing uncertain trends. Conventional prediction approaches have built multiple growth models that classify similar growth patterns. However, it is not easy to classify
            physical growth pattern when the overall trend is unknown. In this paper, we present a predictive analysis technique to forecast physical growth over time. The technique is a relative data-driven approach that explores similar data and weaves
            them to create approximate inference margins. We also propose a visual analytics to analyze physical growth trends.</talk-abstract>
        </talk>
        <talk>
          <talk-title>Optimization of Motorcycle Riders Categorization Based on Emotion Using Decision Tree Analysis</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>kodai obata</author-name> (
            <affiliation>Kwansei Gakuin University, JAPAN</affiliation>)</author>
          <author>
            <author-name>Masashi Sugimoto</author-name> (
            <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
          <author>
            <author-name>Noriko Nagata</author-name> (
            <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
          <talk-abstract>In the present research, we optimized emotional rider categorization using decision tree analysis. First, we asked participants to evaluate four emotions (“enjoyable/brisk,” “pleasant/comfortable,” “boring/unsatisfying,” and “uneasy/scary”)
            toward 90 motorcycle pictures. Then, we reduced the amount of evaluation times (from two to seven evaluation). The optimized model succeeded in categorization with 85% accuracy. The optimized model also succeeded in replicating the emotion
            pattern of the clusters in the original model. The results indicate the high validity of the optimized model. </talk-abstract>
        </talk>
        <talk>
          <talk-title>Landscape Simulation System to Inherit the History of the Local Area</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Yasuo Kawai</author-name> (
            <affiliation>Bunkyo University, Japan</affiliation>)</author>
          <author>
            <author-name>Yurika Tsuchiya</author-name> (
            <affiliation>Bunkyo University, Japan</affiliation>)</author>
          <author>
            <author-name>Youhei Obuchi</author-name> (
            <affiliation>Bunkyo University, Japan</affiliation>)</author>
          <author>
            <author-name>Shoko Nihei</author-name> (
            <affiliation>Bunkyo University, Japan</affiliation>)</author>
          <talk-abstract>We present a landscape simulation system that reproduces the post town in the late Edo period of Japan. This simulation system elucidates the historical culture inheritance of the local area. The study area for this system was Fujisawa-jyuku
            in the late Edo period, which was a post town. The previous historical landscape reproduction system is currently comprised of a renowned building and its place. This system can be cost-effectively and easily developed by using a game engine.
            We designed to recreate the town, referencing Ukiyo-e, old documents and maps. It can be used as a tool for residents to learn about their regional history.</talk-abstract>
        </talk>
        <talk>
          <talk-title>Experimental Study of Natural Interaction in InfoVis and Gesture Design</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>Ming Zhang</author-name> (
            <affiliation>Product Design, Faculty of Art and Design, University of Tsukuba, Japan</affiliation>;
            <affiliation>School of Digital Media &amp; Design Arts, Beijing University of Posts &amp; Telecommunications, Beijing, Beijing, China</affiliation>)</author>
          <author>
            <author-name>Tiemeng Li</author-name> (
            <affiliation>Digital Media &amp; Design Art School. Beijing University of Posts &amp; Telecommunications., China</affiliation>)</author>
          <talk-abstract>This work mainly studied the users on how they interact with the data, how they use gestures and how they understand the data by conducting the experiments. Meanwhile, based on the current parallel coordinate system of information visualization,
            we compared the gesture interaction and the mouse interaction on their effective, efficient, workload and usability. Through the experiments, we conclude user-defined gesture set. In addition, we optimized the gesture interaction of the current
            parallel coordinate system based on the results of our experiment.</talk-abstract>
        </talk>
        <talk>
          <talk-title>Challenges in Visualizing Complex Causality Characteristics</talk-title>
          <talk-type class="Poster">Poster</talk-type>
          <author>
            <author-name>William Wright</author-name> (
            <affiliation>Uncharted Software, Canada</affiliation>)</author>
          <author>
            <author-name>Thomas Kapler</author-name> (
            <affiliation>Uncharted Software, Canada</affiliation>)</author>
          <talk-abstract>This poster paper examines current methods of visualizing causality and their limitations. Causality is important for providing explanations especially when using computational models to understand complex systems structure and behavior, and
            what happens when change occurs in the system. There are many properties of causality that need to be considered and made visible, but current causality visualization methods are limited in expressions, scale, dimensionality and do not provide
            sufficient support for user tasks such as “what-if” and “how-to’ questions, or in supporting groups considering multiple scenarios. There are many challenges to be discussed.</talk-abstract>
        </talk>
      </p>

      <section class="container marketing lead" align="justify" level="3">
        <h3 id="visual-data-storytelling-contest-1">Visual Data Storytelling Contest #1</h3>
        <p>
          <talk>
            <talk-title><a href="http://hshidara.github.io/PVis-Contest/">North Korea: Real or Paper Tiger?</a></talk-title>
            <talk-type class="Contest">Contest</talk-type>
          </talk>
        </p>
        <p>
          <author>
            <author-name>Hidekazu Shidara</author-name> (
            <affiliation>University of California Davis</affiliation>)</author>
          <author>
            <author-name> Chris Bryan</author-name> (
            <affiliation>University of California Davis</affiliation>)</author>
          <author>
            <author-name> Oh-Hyun Kwon</author-name> (
            <affiliation>University of California Davis</affiliation>)</author>
          <author>
            <author-name> Kwan-Liu Ma</author-name> (
            <affiliation>University of California Davis</affiliation>)</author>
          <talk-abstract>As we enter 2018, is North Korea a real or paper tiger? To investigate this question, we present a series of interactive visualizations about the Korean peninsula using publicly available datasets and published news stories in a data-driven,
            slideshow format. We explore both sides of the issue, presenting data and provoking the reader to form their own conclusions. What does the future hold for this secretive nation? Are they marching inexorably towards a military confrontation?
            Or, are they ultimately ineffectual and powerless? </talk-abstract>
        </p>
        <p>
          <talk>
            <talk-title><a href="https://vis-stdio.hanu.io/pacificvis/music.html">Music Tailor: Data Visualization of Sparkline and Multi-Attribute Ranking</a></talk-title>
            <talk-type class="Contest">Contest</talk-type>
          </talk>
        </p>
        <p>
          <author>
            <author-name>Soojung Lee</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Jaeho Han</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Taeyeong Kil</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Piljun Kim</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Hyunwoo Han</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Kyungwon Lee</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <talk-abstract>Gaon chart is a reliable chart used in the Korean music broadcasting rank and a key reference for the selection of music by audience. However, Gaon Chart decides the rank by limiting music to property values, so audience can’t view accurate
            information and the chart of the standard they want. If the user can create a chart of their choice, they can compare it to the original chart with a different weight. This music chart is based on the number of downloads, streaming and SNS
            mentions on the Gaon chart. The user can configure the chart by adjusting the weight of the attribute. Through changing the ranking of attributes for 5 weeks, it is possible to observe the tendency of music and the flow of the music market.
            In the left layout, you can see the characteristics of the song by showing the detailed flowchart.</talk-abstract>
        </p>
        <p>
          <talk>
            <talk-title><a href="https://visprojects.hanu.io/pacificvis/photointhecity.html">Photo in the City: Identifying the Color and the Theme of Instagram Photos Taken in Different Cities</a></talk-title>
            <talk-type class="Contest">Contest</talk-type>
          </talk>
        </p>
        <p>
          <author>
            <author-name>Hyunsik Gong</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Yejin Kim</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Juwon Hong</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Hyerim Joung</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Hyunwoo Han</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Kyungwon Lee</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <contest-vimeo><iframe src="https://player.vimeo.com/video/258174240" width="640" height="400" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe></contest-vimeo>
        </p>
        <p>
          <talk-abstract>We mainly focused on developing an interactive visualization system using photo data from Instagram. We collected photographs of major cities in six different countries uploaded from May 2017 to December 2017 by searching with hashtags including
            country and city name from Instagram. (e.g. #TokyoJapan). Then we used Google Vision API to extract information about the topics and colors that each photo contains. The result of our visualization project would give proper answers to users
            who have questions such as “Are there any discernible topics in a city compared to others?” or “What is most dominant color in each city’s photo data?” A visualization technique enables users to figure out the difference of certain topics
            among cities and the distribution of colors in the data on particular themes for each city.</talk-abstract>
        </p>
        <p>
          <talk>
            <talk-title><a href="https://visprojects.hanu.io/pacificvis/gun.html">Is gun laws more adopted, safer?</a></talk-title>
            <talk-type class="Contest">Contest</talk-type>
          </talk>
        </p>
        <p>
          <author>
            <author-name>Jaejong Heo</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Cheongbin Kim</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Hyunwoo Han</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <author>
            <author-name>Kyungwon Lee</author-name> (
            <affiliation>Ajou University</affiliation>)</author>
          <talk-abstract>In Las Vegas, Nevada, mass casualties occurred, resulting in numerous casualties. We can look at the types of firearm accidents that happen frequently in US states. In order to find the cause of frequent firearm accidents, we examine characteristics
            of each state. To explain the visualization in detail, On the right side of the circle, the number of victims classified in the US states, the number of victims classified according to the type of gun incidents’ on the left side of the circle
            and The right outer stack graph shows gun control legislation. This design allows us to compare data in size, color and sort order of USA’s states index.</talk-abstract>
        </p>
        <p>
          <talk>
            <talk-title>A Travel of the Metabolite</talk-title>
            <talk-type class="Contest">Contest</talk-type>
          </talk>
        </p>
        <p>
          <author>
            <author-name>Hsiang-Yun Wu</author-name> (
            <affiliation>Technische Universität Wien</affiliation>)</author>
          <author>
            <author-name>Martin Nöllenburg</author-name> (
            <affiliation>Technische Universität Wien</affiliation>)</author>
          <author>
            <author-name>Ivan Viola</author-name> (
            <affiliation>Technische Universität Wien</affiliation>)</author>
          <contest-vimeo><iframe src="https://player.vimeo.com/video/258174044" width="640" height="480" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe></contest-vimeo>
        </p>
        <p>
          <talk-abstract>Biological pathways are chains of molecule interactions and reactions in biological systems that jointly form complex, hierarchical networks. Although several pathway layout algorithms have been investigated, biologists still prefer to use hand-drawn
            ones, due to their higher visual quality. In this project, we propose a visualization for computing metabolic pathway maps that emphasize the underlying grouping structure of pathway datasets and use orthogonal-style edge routing to simplify
            edge orientation. This idea is inspired by concepts from urban planning, where we consider reactions as city blocks and built up roads to connect identical metabolites occurred in different biological categories. We provide a story to present
            how glucose is broken down to phosphoenolpyruvate in glycolysis-gluconeogenesis to release energy, which is often stored in adenosine triphosphate (ATP) in a human body. Finally, how ATP can be utilized to synthesize urea in the urea cycle
            pathways is demonstrated.</talk-abstract>
        </p>
      </section>
    </section>

    <section class="container marketing lead" align="justify" level="2">
      <h2 id="Session-1-Graphs-and-Networks">Session 1: Graphs and Networks</h2>
      <p>
        <session-start>April 11 (Wednesday) 14:40</session-start>
        <session-chair>Chair: Tim Dwyer (
          <affiliation>Monash University</affiliation>)</session-chair>
      </p>
      <talk>
        <talk-title>Optimal Algorithms for Compact Linear Layouts</talk-title>
        <talk-type class="Paper">Paper</talk-type>
        <author>
          <author-name>Willem Sonke</author-name> (
          <affiliation>TU Eindhoven, Netherlands</affiliation>)</author>
        <author>
          <author-name>Kevin Verbeek</author-name> (
          <affiliation>TU Eindhoven, Netherlands</affiliation>)</author>
        <author>
          <author-name>Wouter Meulemans</author-name> (
          <affiliation>TU Eindhoven, Netherlands</affiliation>)</author>
        <author>
          <author-name>Eric Verbeek</author-name> (
          <affiliation>TU Eindhoven, Netherlands</affiliation>)</author>
        <author>
          <author-name>Bettina Speckmann</author-name> (
          <affiliation>TU Eindhoven, Netherlands</affiliation>)</author>
        <talk-abstract>Linear layouts are a simple and natural way to draw a graph: all vertices are placed on a single line and edges are drawn as arcs between the vertices. Despite its simplicity, a linear layout can be a very meaningful visualization if there is
          a particular order defined on the vertices. Common examples of such ordered - and often also directed - graphs are event sequences and processes. A main drawback of linear layouts are the usually (very) large aspect ratios of the resulting drawings,
          which prevent users from obtaining a good overview of the whole graph. &nbsp; &nbsp;In this paper we present a novel and versatile algorithm to optimally fold a linear layout of a graph such that it can be drawn nicely in a specified aspect
          ratio, while still clearly communicating the linearity of the layout. Our algorithm allows vertices to be drawn as blocks or rectangles of specified sizes to incorporate different drawing styles, label sizes, and even recursive structures. For
          reasonably-sized drawings the folded layout can be computed interactively. We demonstrate the applicability of our algorithm on graphs that represent process trees, a particular type of process model. Our algorithm arguably produces much more
          readable layouts than existing methods.</talk-abstract>
      </talk>
      <talk>
        <talk-title>BC Tree based Proxy Graphs for Visualization of Big Graphs</talk-title>
        <talk-type class="Paper">Paper</talk-type>
        <author>
          <author-name>Seokhee Hong</author-name> (
          <affiliation>The University of Sydney, Australia</affiliation>)</author>
        <author>
          <author-name>Quan Nguyen</author-name> (
          <affiliation>The University of Sydney, Australia</affiliation>)</author>
        <author>
          <author-name>Amyra Meidiana</author-name> (
          <affiliation>University of Sydney, Australia</affiliation>)</author>
        <author>
          <author-name>Jiaxi Li</author-name> (
          <affiliation>The University of Sydney, Australia</affiliation>)</author>
        <author>
          <author-name>Peter Eades</author-name> (
          <affiliation>The University of Sydney, Australia</affiliation>)</author>
        <talk-abstract>Recent work for visualizing big graphs uses a proxy graph approach: the original graph is replaced by a proxy graph, which is much smaller than the original graph. The challenge for the proxy graph approach is to ensure that the proxy graph is
          a good representation of the original graph. However, previous work to compute proxy graphs using graph sampling techniques often fails to preserve connectivity and important global skeletal structure in the original graph.
          <p>
            This paper introduces two new families of proxy graph methods BCP-W and BCP-E, tightly integrating graph sampling methods with the BC (Block Cut-vertex) tree, which represents the decomposition of a graph into biconnected components. Experimental results
            using graph sampling quality metrics show that our new BC tree-based proxy graph methods produce significantly better results than existing sampling-based proxy graph methods: 25% improvement by BCP-W and 15% by BCP-E on average. We also present
            DBCP, a BC tree-based proxy graph method for distributed environment. Experiments on the Amazon Cloud EC2 demonstrate that DBCP is scalable for big graph data sets; runtime speed-up of 77% for distributed 5-server on average.
          </p>
          <p>
          </p>
          <p>Visual comparison using a graph layout method and the proxy quality metrics confirm that our new BC tree-based proxy graph methods are significantly better than existing sampling-based proxy graph method. Our main results lead to guidelines
            for computing sampling-based proxy graphs for visualization of big graphs.
            <talk>
              <talk-title>Development of an Integrated Visualization System for Phenotype Character Networks</talk-title>
              <talk-type class="Note">Note</talk-type>
              <author>
                <author-name>Yosuke Onoue</author-name> (
                <affiliation>Kyoto University</affiliation>)</author>
              <author>
                <author-name>Koji Kyoda</author-name> (
                <affiliation>RIKEN</affiliation>)</author>
              <author>
                <author-name>Miki Kioka</author-name> (
                <affiliation>Kyoto University</affiliation>)</author>
              <author>
                <author-name>Kazutaka Baba</author-name> (
                <affiliation>Kyoto University</affiliation>)</author>
              <author>
                <author-name>Shuichi Onami</author-name> (
                <affiliation>RIKEN</affiliation>)</author>
              <author>
                <author-name>Koji Koyamada</author-name> (
                <affiliation>Kyoto University</affiliation>)</author>
              <talk-abstract>Wet and dry biological data are potentially complementary. By visually integrating the initiation and developmental processes of organisms, we might reveal new causalities in biological data. Here we present an integrated visualization system
                for a causality network constructed from phenotypic developmental characters and their related scientific literature. To obtain the phenotypic characters, we applied bio-imaging informatics techniques to the data of wet experiments. The
                phenotypic character network was visually rendered in the CausalNet system, which provides both explanatory and verification visualization functions. Statistical analysis and scientific literature mining proved useful for determining the
                mechanisms underlying the phenotypic trait network. The validity of the system was confirmed in an application example and expert feedback on the developmental process of the nematode Caenorhabditis elegans. The discussed methodology is
                applicable to other multicellular organisms.</talk-abstract>
            </talk>
          </p>
          <h2 id="Session-2-Volume-Visualization">Session 2: Volume Visualization</h2>
          <p>
            <session-start>April 11 (Wednesday) 16:00</session-start>
            <session-chair>Chair: Chuck Hansen (
              <affiliation>University of Utah</affiliation>)</session-chair>
          </p>
          <p>
            <talk>
              <talk-title>Image and Distribution Based Volume Rendering for Large Data Sets</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Ko-Chih Wang</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <author>
                <author-name>Naeem Shareef</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <author>
                <author-name>Han-Wei Shen</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <talk-abstract>Analyzing scientific datasets created from simulations on modern supercomputers is a daunting challenge due to the fast pace at which these datasets continue to grow. Low cost post analysis machines used by scientists to view and analyze
                these massive datasets are severely limited by their deficiencies in storage bandwidth, capacity, and computational power. Trying to simply move these datasets to these platforms is infeasible. Any approach to view and analyze these datasets
                on post analysis machines will have to effectively address the inevitable problem of data loss. Image based approaches are well suited for handling very large datasets on low cost platforms. Three challenges with these approaches are how
                to effectively represent the original data with minimal data loss, analyze the data in regards to transfer function exploration, which is a key analysis tool, and quantify the error from data loss during analysis. We present a novel image
                based approach using distributions to preserve data integrity. At each view sample, view dependent data is summarized at each pixel with distributions to define a compact proxy for the original dataset. We present this representation along
                with how to manipulate and render large scale datasets on post analysis machines. We show that our approach is a good trade off between rendering quality and interactive speed and provides uncertainty quantification for the information
                that is lost.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Smart Surrogate Widgets for Direct Volume Manipulation</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Sergej Stoppel</author-name> (
                <affiliation>University of Bergen, Norway</affiliation>)</author>
              <author>
                <author-name>Stefan Bruckner</author-name> (
                <affiliation>University of Bergen, Norway</affiliation>)</author>
              <talk-abstract>Interaction is an essential aspect in volume visualization, yet common &nbsp;manipulation tools such as bounding boxes or clipping plane &nbsp;widgets provide rather crude tools as they neglect the complex structure &nbsp;of the underlying
                data. In this paper, we introduce a novel &nbsp;volume interaction approach based on smart widgets that are automatically &nbsp;placed directly into the data in a visibility-driven manner. &nbsp;By adapting to what the user actually sees,
                they act as proxies that &nbsp;allow for goal-oriented modifications while still providing an intuitive &nbsp;set of simple operations that is easy to control. In particular, our &nbsp;method is well-suited for direct manipulation scenarios
                such as touch &nbsp;screens, where traditional user interface elements commonly exhibit &nbsp;limited utility. To evaluate out approach we conducted a qualitative &nbsp;user study with nine participants with various backgrounds.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Topologically Controlled Lossy Compression</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Maxime Soler</author-name> (
                <affiliation>Total S.A., France</affiliation>)</author>
              <author>
                <author-name>Melanie Plainchault</author-name> (
                <affiliation>Total SA, France</affiliation>)</author>
              <author>
                <author-name>Bruno Conche</author-name> (
                <affiliation>Total SA, France</affiliation>)</author>
              <author>
                <author-name>Julien Tierny</author-name> (
                <affiliation>CNRS LIP6, UPMC, Sorbonne Universites, France</affiliation>)</author>
              <talk-abstract>This paper presents a new algorithm for the lossy compression of scalar data defined on 2D or 3D regular grids, with topological control. Certain techniques allow users to control the pointwise error induced by the compression. However,
                in many scenarios it is desirable to control in a similar way the preservation of higher-level notions, such as topological features, in order to provide guarantees on the outcome of post-hoc data analyses. This paper presents the first
                compression technique for scalar data which supports a strictly controlled loss of topological features. It provides users with specific guarantees both on the preservation of the important features and on the size of the smaller features
                destroyed during compression. In particular, we present a simple compression strategy based on a topologically adaptive quantization of the range. Our algorithm provides strong guarantees on the bottleneck distance between persistence
                diagrams of the input and decompressed data, specifically those associated with extrema. A simple extension of our strategy additionally enables a control on the pointwise error. We also show how to combine our approach with state-of-the-art
                compressors, to further improve the geometrical reconstruction. Extensive experiments, for comparable compression rates, demonstrate the superiority of our algorithm in terms of the preservation of topological features. We show the utility
                of our approach by illustrating the compatibility between the output of post-hoc topological data analysis pipelines, executed on the input and decompressed data, for simulated or acquired data sets. We also provide a lightweight VTK-based
                C++ implementation of our approach for reproduction purposes.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Information Guided Data Sampling and Recovery using Bitmap Indexing</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Tzu-Hsuan Wei</author-name> (
                <affiliation>The Ohio State University, USA</affiliation>)</author>
              <author>
                <author-name>Soumya Dutta</author-name> (
                <affiliation>The Ohio State University, USA</affiliation>)</author>
              <author>
                <author-name>Han-Wei Shen</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <talk-abstract>Creating a data representation is a common approach for efficient &nbsp;and effective data management and exploration. The compressed &nbsp;bitmap indexing is one of the emerging data representation used for &nbsp;large-scale data exploration.
                Performing sampling on the bitmapindexing &nbsp;based data representation allows further reduction of storage &nbsp;overhead and be more flexible to meet the requirements of &nbsp;different applications. In this paper, we propose two approaches
                &nbsp;to solve two potential limitations when exploring and visualizing &nbsp;the data using sampling-based bitmap indexing data representation. &nbsp;First, we propose an adaptive sampling approach called information &nbsp;guided stratified
                sampling (IGStS) for creating compact sampled &nbsp;datasets that preserves the important characteristics of the raw data. &nbsp;Furthermore, we propose a novel data recovery approach to reconstruct &nbsp;the irregular subsampled dataset
                into a volume dataset with &nbsp;regular grid structure for qualitative post-hoc data exploration and &nbsp;visualization. The quantitative and visual efficacy of our proposed &nbsp;data sampling and recovery approaches are demonstrated
                through &nbsp;multiple experiments and applications.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Multiresolution Volume Filtering in the Tensor Compressed Domain</talk-title>
              <talk-type class="TVCG">TVCG</talk-type>
              <author>
                <author-name>Rafael Ballester-Ripoll</author-name> (
                <affiliation>Department of Informatics, University of Zurich, Switzerland</affiliation>)</author>
              <author>
                <author-name>David Steiner</author-name> (
                <affiliation>Department of Informatics, University of Zurich, Switzerland</affiliation>)</author>
              <author>
                <author-name>Renato Pajarola</author-name> (
                <affiliation>Department of Informatics, University of Zurich, Switzerland</affiliation>)</author>
              <talk-abstract>Signal processing and filter operations are important tools for visual data processing and analysis. Due to GPU memory and bandwidth limitations, it is challenging to apply complex filter operators to large-scale volume data interactively.
                We propose a novel and fast multiscale compression-domain volume filtering approach integrated into an interactive multiresolution volume visualization framework. In our approach, the raw volume data is decomposed offline into a compact
                hierarchical multiresolution tensor approximation model. We then demonstrate how convolution filter operators can effectively be applied in the compressed tensor approximation domain. To prevent aliasing due to multiresolution filtering,
                our solution (a) filters accurately at the full spatial volume resolution at a very low cost in the compressed domain, and (b) reconstructs and displays the filtered result at variable level-of-detail. The proposed system is scalable,
                allowing interactive display and filtering of large volume datasets that may exceed the available GPU memory. The desired filter kernel mask and size can be modified online, producing immediate visual results.</talk-abstract>
            </talk>
          </p>
          <h2 id="Session-3-Particles-Molecules-and-Uncertainty">Session 3: Particles, Molecules, and Uncertainty</h2>
          <p>
            <session-start>April 12 (Thursday) 11:00</session-start>
            <session-chair>Chair: Barbora Kozlíková (
              <affiliation>Masaryk University</affiliation>)</session-chair>
          </p>
          <p>
            <talk>
              <talk-title>In Situ Prediction Driven Feature Analysis in Jet Engine Simulations</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Soumya Dutta</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <author>
                <author-name>Han-Wei Shen</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <author>
                <author-name>Jenping Chen</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <talk-abstract>Efficient feature exploration in large-scale data sets using traditional post-hoc analysis approaches is becoming prohibitive due to the bottleneck stemming from I/O and output data sizes. This problem becomes more challenging when an ensemble
                of simulations are required to run for studying the influence of input parameters on the model output. As a result, scientists are inclining more towards analyzing the data in situ while it resides in the memory. In situ analysis aims
                at minimizing expensive data movement while maximizing the resource utilization for extraction of important information from the data. In this work, we study the evolution of rotating stall in jet engines using data generated from a large-scale
                flow simulation under various input conditions. Since the features of interest lack a precise descriptor, we adopt a fuzzy rule-based machine learning algorithm for efficient and robust extraction of such features. For scalable exploration,
                we advocate for an off-line learning and in situ prediction driven strategy that facilitates in-depth study of the stall. Task-specific information estimated in situ is visualized interactively during the post-hoc analysis revealing important
                details about the inception and evolution of stall. We verify and validate our method through comprehensive expert evaluation demonstrating the efficacy of our approach.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Access Pattern Learning with Long Short-term Memory for Parallel Particle Tracing</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Fan Hong</author-name> (
                <affiliation>Peking University, China</affiliation>)</author>
              <author>
                <author-name>Jiang Zhang</author-name> (
                <affiliation>Peking University, China</affiliation>)</author>
              <author>
                <author-name>Xiaoru Yuan</author-name> (
                <affiliation>Peking University, China</affiliation>)</author>
              <talk-abstract>In this work, we present a novel access pattern estimation approach for parallel particle tracing in flow field visualization based on deep neural networks. With strong generalization ability, we develop a Long Short-term Memory (LSTM)-based
                model, which is capable of learning accurate access patterns with only a few training samples and representing the learned patterns with small storage overhead. Equipped with prediction and prefetching functions driven by the developed
                model, our parallel particle tracing framework employs CPUs and GPUs together for particle tracing tasks. We demonstrate the accuracy and time efficiency of our approach with various flow visualization applications in three different flow
                datasets. &nbsp;</talk-abstract>
            </talk>
            <talk>
              <talk-title>Dynamic Data Repartitioning for Load-Balanced Parallel Particle Tracing</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Jiang Zhang</author-name> (
                <affiliation>Peking University, China</affiliation>)</author>
              <author>
                <author-name>Hanqi Guo</author-name> (
                <affiliation>Argonne National Laboratory, United States</affiliation>)</author>
              <author>
                <author-name>Xiaoru Yuan</author-name> (
                <affiliation>Peking University, China</affiliation>)</author>
              <author>
                <author-name>Tom Peterka</author-name> (
                <affiliation>Argonne National Laboratory, United States</affiliation>)</author>
              <talk-abstract>We present a novel dynamic load-balancing algorithm based on data repartitioning for parallel particle tracing in flow visualization. Instead of static data assignment, we dynamically repartition the data into blocks and reassign the blocks
                to processes to balance the workload distribution among the processes. Block repartitioning is performed based on a dynamic workload estimation method that predicts the workload in the flow field on the fly as the input. In our approach,
                we allow data duplication in the repartitioning, enabling the same data blocks to be assigned to multiple processes. Load balance is achieved by regularly exchanging the blocks (together with the particles in the blocks) among processes
                according to the output of the data repartitioning. Compared with other load-balancing algorithms, our approach does not need any preprocessing on the raw data and does not require any dedicated process for work scheduling, while it has
                the capability to balance uneven workload efficiently. Results show improved load balance and high efficiency of our method on tracing particles in both steady and unsteady flow.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Uncertainty Visualization for Secondary Structures of Proteins</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Christoph Schulz</author-name> (
                <affiliation>University of Stuttgart, Germany</affiliation>)</author>
              <author>
                <author-name>Karsten Schatz</author-name> (
                <affiliation>University of Stuttgart, Germany</affiliation>)</author>
              <author>
                <author-name>Michael Krone</author-name> (
                <affiliation>University of Stuttgart, Germany</affiliation>)</author>
              <author>
                <author-name>Matthias Braun</author-name> (
                <affiliation>University of Stuttgart, Germany</affiliation>)</author>
              <author>
                <author-name>Thomas Ertl</author-name> (
                <affiliation>University of Stuttgart, Germany</affiliation>)</author>
              <author>
                <author-name>Daniel Weiskopf</author-name> (
                <affiliation>University of Stuttgart, Germany</affiliation>)</author>
              <talk-abstract>We present a technique that conveys the uncertainty in the secondary structure of proteins-an abstraction model based on atomic coordinates. &nbsp;While protein data inherently contains uncertainty due to the acquisition method or the simulation
                algorithm, we argue that it is also worth investigating uncertainty induced by analysis algorithms that precede visualization. &nbsp;Our technique helps researchers investigate differences between multiple secondary structure assignment
                methods. &nbsp;We modify established algorithms for fuzzy classification and introduce a discrepancy-based approach to project an ensemble of sequences to a single importance-weighted sequence. &nbsp;In 2D, we depict the aggregated secondary
                structure assignments based on the per-residue deviation in a collapsible sequence diagram. &nbsp;In 3D, we extend the ribbon diagram using visual variables such as transparency, wave form, frequency, or amplitude to facilitate qualitative
                analysis of uncertainty. &nbsp;We evaluated the effectiveness and acceptance of our technique through expert reviews using two example applications: the combined assignment against established algorithms and time-dependent structural changes
                originating from simulated protein dynamics. &nbsp;</talk-abstract>
            </talk>
            <talk>
              <talk-title>Modeling and Visualization of Uncertainty-aware Geometries using Multi-variate Normal Distributions</talk-title>
              <talk-type class="Note">Note</talk-type>
              <author>
                <author-name>Christina Gillmann</author-name> (
                <affiliation>University of Kaiserslautern, Kaiserslauten, Rheinland-Pfalz, Germany</affiliation>)</author>
              <author>
                <author-name>Thomas Wischgoll</author-name> (
                <affiliation>Wright State University, Dayton, Ohio, United States</affiliation>)</author>
              <author>
                <author-name>Bernd Hamann</author-name> (
                <affiliation>University of California, Davis, Davis, California, U.S.A.</affiliation>)</author>
              <author>
                <author-name>James Ahrens</author-name> (
                <affiliation>Los Alamos National Laboratory, Los Alamos, New Mexico, United States</affiliation>)</author>
              <talk-abstract>Many applications are dealing with geometric data that are affected by uncertainty. It is important to analyze, visualize, and understand the properties of uncertain geometry. We present a methodology to model uncertain geometry based on
                multi-variate normal distributions. In addition, we propose a visualization technique to represent a hull for uncertain geometry capturing a user-defined percentage of the underlying uncertain geometry. To show the effectiveness of our
                approach, we have modeled and visualized uncertain datasets from different applications.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Visualization of Fullerene Fragmentation</talk-title>
              <talk-type class="Note">Note</talk-type>
              <author>
                <author-name>Kai Sdeo</author-name> (
                <affiliation>Heidelberg University, 69120 Heidelberg, Baden-Wuerttemberg, Germany</affiliation>)</author>
              <author>
                <author-name>Bastian Rieck</author-name> (
                <affiliation>Heidelberg University, 69120 Heidelberg, Baden-Wuerttemberg, Germany</affiliation>)</author>
              <author>
                <author-name>Filip Sadlo</author-name> (
                <affiliation>Heidelberg University, 69120 Heidelberg, Baden-Wuerttemberg, Germany</affiliation>)</author>
              <talk-abstract>In this paper, we present a novel visualization approach for the analysis of fragmentation of molecules, with a particular focus on fullerenes. Our approach consists of different components at different levels of detail. Whereas one component
                is geometric but invariant to rotations, two other components are based on the topological structure of the molecules and thus additionally invariant to deformations. By combining these three components, which aim at the analysis of simulation
                ensembles of such molecules, and complementing them with a space–time representation that enables detailed interactive inspection of individual simulations, we obtain a versatile tool for the analysis of the fragmentation of structured,
                symmetrical molecules such as fullerenes. We exemplify the utility of our approach using a tightly coupled simulation approach for the dynamics of fullerenes.</talk-abstract>
            </talk>
          </p>
          <h2 id="Interactive-Session-2-">Interactive Session 2:</h2>
          <p>
            <session-start>April 12 (Thursday) 12:40</session-start>
            <talk>
              <talk-title>Proposal of visual analytics interface for time series data employing trajectory manipulation</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Rei Takami</author-name> (
                <affiliation>Tokyo Metropolitan University, Japan</affiliation>)</author>
              <author>
                <author-name>Yasufumi Takama</author-name> (
                <affiliation>Tokyo Metropolitan University, Japan</affiliation>)</author>
              <talk-abstract>Recently, various time series data have been collected in many fields, and visual analytics interface is expected to be useful for utilizing such data. However, several issues arising from the property of time series data should be considered
                when developing a visual analytics interface. For example, when time series data is visualized using animation, collision would occur between movement of time series data itself and movement caused by interaction with users. In order to
                solve those issues, this paper proposes a visual analytics interface for time series data based on trajectory manipulation, which can handle temporal and spatial changes uniformly. A part of the experimental results with test participants
                is reported in the paper.</talk-abstract>
            </talk>
            <talk>
              <talk-title>A Visualization System for Data Anomaly Detection in IoT Applications</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Fan Zhao</author-name> (
                <affiliation>Xinjiang Technical Institute of Physics and Chemistry,Chinese Academy of Sciences, China</affiliation>)</author>
              <author>
                <author-name>TongHai Jiang</author-name> (
                <affiliation>Xinjiang Technical Institute of Physics and Chemistry,Chinese Academy of Sciences, China</affiliation>)</author>
              <author>
                <author-name>Li Cheng</author-name> (
                <affiliation>Xinjiang Technical Institute of Physics and Chemistry,Chinese Academy of Sciences, China</affiliation>)</author>
              <talk-abstract>Analysis on urban vehicle behavior patterns has increasingly drawn researcher’s attention due to its importance in transportation, urban management and public safety. Among various available vehicle activity data, drivers’ refueling behavior
                is particularly important in the context of public safety. In this research, we develop an interactive visualization analysis system which can be applied in abnormal behavior detection for public safety monitoring agencies. Through the
                analysis of gas stations, vehicles and drivers, we extract proper features and generalize common refueling behavior patterns for a province-wide region. We adopt five different visualization models which take various data features and
                patterns into account, including sparse feature, geospatial-temporal feature and multi-dimensional feature. Our visualization system supports interactive user operations to retrieve, display and analyze individual or group vehicle behaviors.
                Users can further obtain the statistical group features and typical individual behavior patterns in order to identify abnormal events. Experiment results strongly supports the validity and practical feasibility of our system in the domain
                of public safety monitoring.</talk-abstract>
            </talk>
            <talk>
              <talk-title>The Integration of Visual and Haptic Impressions Felt from Synthetic Resin Texture</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Taishi Fujiwara</author-name> (
                <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
              <author>
                <author-name>Atsushi Takemoto</author-name> (
                <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
              <author>
                <author-name>Yusuke TANI</author-name> (
                <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
              <author>
                <author-name>Kensuke Tobitani</author-name> (
                <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
              <author>
                <author-name>Noriko Nagata</author-name> (
                <affiliation>Kwansei Gakuin University, Japan</affiliation>)</author>
              <talk-abstract>The ability of texture perception is important to estimate the materials and properties of objects. However, the ways how multimodal information can be applied to texture recognition has yet to be fully elucidated. In this research study,
                we modeled the relationship among visual, haptic, and visuo-haptic impressions using multiple-regressions analysis. Some of the models demonstrated that the product of scored visual and haptic impressions plays an important role in multimodal
                texture perception. This result suggests that a multimodal texture perception can greatly change the impressions of texture more than we imagined.</talk-abstract>
            </talk>
            <talk>
              <talk-title>3D Visualization of Versioning Graph</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Ko Fujimura</author-name> (
                <affiliation>Otsuma Women’s University, Japan</affiliation>)</author>
              <talk-abstract>This paper proposes modeling of development processes in cooperative work as a versioning graph that consists of contribution edges and update edges. This modeling is suitable for visualizing the process as animation in which new nodes and
                edges are added to the scene over time. This paper also presents the basic idea of the visualizing method of the versioning graph in 3D space. We found that 3D representation is suitable for understanding the process of the development
                of the cooperative work. We implemented this visualization method and applied to some open source communities. This tool, GitVis3D, and its demonstration are available from our git repository.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Visualization of diffusion behavior patterns on Twitter</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Chisae Iwashina</author-name> (
                <affiliation>Ochanomizu University , Japan</affiliation>)</author>
              <author>
                <author-name>Mitsuo Yoshida</author-name> (
                <affiliation>Toyohashi University of Technology, Japan</affiliation>)</author>
              <author>
                <author-name>Takayuki Itoh</author-name> (
                <affiliation>Ochanomizu University, Japan</affiliation>)</author>
              <talk-abstract>There is the increasing number of examples of using SNS that is used as information transmission tool for the purpose of spreading information by organizations and individuals. Among SNSs, Twitter is particularly known to be able to spread
                out easily (retweet), its use is still active. Entertainers, fashion brands, enterprises and others use this function to notify information. Therefore, there are influential users of various genres on Twitter. One of the major reasons
                that these users exist is that there are many users who spread information (retweets). In this study, we focused attention on users who have influential users and expressed the relationship between the tweet group of the influential users
                and the surrounding user group by a visualization method using dendrogram and heat map. As to the nature of information spreading, it is found that the surrounding user layer and the number of people who react in relation to tweet content
                and tweet time are changed by considering both tweet contents and surrounding user ’s aspects.</talk-abstract>
            </talk>
            <talk>
              <talk-title>SeqTextVis: A Distributed Word Representation Learning for Time Sequential Text Data Visualization</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Jheng-Long Wu</author-name> (
                <affiliation>Chinese Culture University, Taiwan</affiliation>;
                <affiliation>Chinese Culture University, Taipei, Taiwan</affiliation>)</author>
              <author>
                <author-name>Mu-Hui Yu</author-name> (
                <affiliation>National Taiwan University, Taiwan</affiliation>;
                <affiliation>National Taiwan University, Taipei, Taiwan</affiliation>)</author>
              <talk-abstract>Texts contain both implicit and explicit information. Explicit info can be easily observed and explored from co-occurrence matrix in text while finding and retrieving implicit info from raw text is very difficult, especially on time sequential
                text data. However, extracting both explicit and implicit info is important because it helps in explaining overall hidden relations among texts. In this paper, we propose SeqTextVis, a new method to learn from time sequential text data,
                and visualize explicit and implicit info based on distributed word representation learning methods: word2vec and node2vec. Word2vec, which is similar to co-occurrence approach, is a simple way to learn neighbor relations and extract explicit
                info; node2vec can extract implicit info by learning parsed structures. SeqTextVis combines with both advantages and time-decay feature to make dynamic graphs of both explicit and implicit relations chronologically. We used case study
                to explain correctness and effectiveness for SeqTextVis system evaluation.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Visual Analysis of the Cut-Point in Urban Public Service</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Lin Zhu</author-name> (
                <affiliation>Tianjin University, China</affiliation>)</author>
              <author>
                <author-name>Ye Yuan</author-name> (
                <affiliation>Tianjin University, China</affiliation>)</author>
              <author>
                <author-name>Chongke Bi</author-name> (
                <affiliation>Tianjin University, China</affiliation>)</author>
              <talk-abstract>Urban public service has become a very important research topic in smart city. Local governments will set up some non-profit or profit-making organizations to provide citizens with convenient and efficient public services. These institutions
                usually include: water, heating, gas, taxi, bus, subway and so on. However, citizens encounter various problems in their daily lives and face great challenges in seeking the services of these organizations. Due to the lack of understanding
                of departmental functions and relationships, citizens cannot find a problem-solving department quickly and accurately after encountering problems. This would lead to slow or even unsolvable problems and seriously affect the quality of
                civic life. In this poster, we proposed a cut-point based visualization scheme for urban public service. The basic idea is to use this visualization scheme to help citizens quickly find the problem-solving departments, shorten the time
                to solve the problem, and improve the quality of civic life.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Visual Style Exploration in Well-designed Diagrams</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Min Lu</author-name> (
                <affiliation>ShenZhen University, China</affiliation>)</author>
              <talk-abstract>Nowadays there are numerous diagrams in good style designed by artists available online, which can serve as the reference to the visualization design. In this work, we present a visual analytic system to explore well-designed diagrams collected
                from Infographics, which support users to retrieve diagrams with specified styler features. Currently we consider the global style features, such as Histogram of Gradients, color histograms, etc. Meanwhile, with image processing method,
                diagrams are decomposed into visual elements, for each of which localized style features will be computed in next step. With the query interface, users are support to query diagrams with both globally and locally specified style features.
                Lastly, we demonstrate the effectiveness of the method by two global query cases</talk-abstract>
            </talk>
            <talk>
              <talk-title>Cell nucleus visualization with phenotypic characteristics</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Sayaka Nagai</author-name> (
                <affiliation>Kobe University, Japan</affiliation>)</author>
              <author>
                <author-name>Naohisa Sakamoto</author-name> (
                <affiliation>Kobe University, Japan</affiliation>)</author>
              <author>
                <author-name>Koji Kyoda</author-name> (
                <affiliation>RIKEN Quantitative Biology Center, Japan</affiliation>)</author>
              <author>
                <author-name>Shuichi Onami</author-name> (
                <affiliation>RIKEN Quantitative Biology Center, Japan</affiliation>)</author>
              <talk-abstract>For elucidating the developmental mechanism of multicellular organisms, it is important to analyze the spatiotemporal features (phenotypic characteristics) of cells appearing during cell division and their correlation. Furthermore, in order
                to analyze how phenotypic characteristics correlate, it is necessary to observe cell nucleus shapes. We proposed a system that visualizes phenotypic characteristics and three-dimensional cell nucleus shapes before, but there were problems
                that cell nucleus shapes cannot be observed individually and information such as cell nuclear position or movement distance was insufficient. In order to solve such problems, we have improved visualization methods of cell nuclei and phenotypic
                characteristics. This improvement enabled us to analyze cell nucleus shapes and a nuclear position accurately.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Information Integrated Visualization System for Heavy Rainfall Risk Analysis</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Masahiko Itoh</author-name> (
                <affiliation>National Institute of Information and Communications Technology, Japan</affiliation>;
                <affiliation>The University of Tokyo, Tokyo, Japan</affiliation>)</author>
              <author>
                <author-name>Takeshi Sagara</author-name> (
                <affiliation>InfoProto Co.,Ltd., Japan</affiliation>)</author>
              <author>
                <author-name>Ukyo Suzuki</author-name> (
                <affiliation>Picolab Co., Ltd., Japan</affiliation>)</author>
              <author>
                <author-name>Kazutoshi Umemoto</author-name> (
                <affiliation>National Institute of Information and Communications Technology, Japan</affiliation>;
                <affiliation>The University of Tokyo, Tokyo, Japan</affiliation>)</author>
              <author>
                <author-name>Naoki Yoshinaga</author-name> (
                <affiliation>The University of Tokyo, Japan</affiliation>)</author>
              <author>
                <author-name>Masashi Toyoda</author-name> (
                <affiliation>The University of Tokyo, Japan</affiliation>)</author>
              <author>
                <author-name>Koji Zettsu</author-name> (
                <affiliation>National Institute of Information and Communications Technology, Japan</affiliation>)</author>
              <author>
                <author-name>Yutaka Kidawara</author-name> (
                <affiliation>National Institute of Information and Communications Technology, Japan</affiliation>)</author>
              <talk-abstract>This paper proposes an information integrated visualization system for heavy rainfall risk analysis. It utilizes multiple sensor data, such as weather data obtained from two kinds of weather radars and event information extracted from Twitter
                data, and visualizes the extracted risks from each sensor data. We show the effectiveness of the system by demonstrating a case study where a risky situation caused by a typhoon was analyzed using real data. The visualization results show
                the typhoon caused many problems, some of which continued after the downpour. Our visualization system is useful to judge risks that cannot be understood by visualization of only one kind of data.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Tsunami Evacuation Behavior Simulation using Agents of Game Engine</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Yasuo Kawai</author-name> (
                <affiliation>Bunkyo University, Japan</affiliation>)</author>
              <author>
                <author-name>Yurie Kaizu</author-name> (
                <affiliation>Bunkyo University, Japan</affiliation>)</author>
              <author>
                <author-name>Kenta Kawahara</author-name> (
                <affiliation>Bunkyo University, Japan</affiliation>)</author>
              <author>
                <author-name>Youhei Obuchi</author-name> (
                <affiliation>Bunkyo University, Japan</affiliation>)</author>
              <author>
                <author-name>Satoshi Otsuka</author-name> (
                <affiliation>Bunkyo University, Japan</affiliation>)</author>
              <author>
                <author-name>Shiori Tomimatsu</author-name> (
                <affiliation>Bunkyo University, Japan</affiliation>)</author>
              <talk-abstract>This study provides an evacuation behavior simulation system at the time of tsunami using the evacuation agent of a game engine. The tsunami evacuation behavior simulation system is a system that enables local governments and residents to
                work together to formulate disaster prevention plans. As a representation of a simple tsunami, in this system, a slightly oblique plane is inserted as the sea surface to a three-dimensional model of the terrain, building, and road, which
                is generated by geographical information system data. Mass agents of evacuees randomly placed on the road will move toward the nearest wide evacuation shelter or tsunami evacuation building. As a result, it was found that serious damage
                will occur even at a tsunami height of 10 m.</talk-abstract>
            </talk>
            <talk>
              <talk-title>To Make Graphs Such As Scatter Plots Numerically Readable</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Toshiyuki Shimono</author-name> (
                <affiliation>Digital Garage, Inc., JAPAN</affiliation>)</author>
              <talk-abstract>Different-sized discrete crosses placed in an organized lattice pattern can assist the human eyes to read numerical values on statistical graphs, enabling more precise interpretation and enlarging the utility of statistical graphs that visually
                represent numerical quantities. This paper presents a novel graph-plotting method that places roughly ten thousand separate grids on a graph, providing human data analysis with an easy access to arbitrary numerical readouts from a statistical
                graph. At present, this functionality has been lacking in the existing graph-plotting software.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Massive Climate Data Visualization for Making Adaptation Policies</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Yosuke Onoue</author-name> (
                <affiliation>Kyoto University, Japan</affiliation>)</author>
              <author>
                <author-name>Takao Naito</author-name> (
                <affiliation>Kyoto University, Japan</affiliation>)</author>
              <author>
                <author-name>Yujin Nakagawa</author-name> (
                <affiliation>JAMSTEC, Japan</affiliation>)</author>
              <author>
                <author-name>Fumiaki Araki</author-name> (
                <affiliation>JAMSTEC, Japan</affiliation>)</author>
              <author>
                <author-name>Koji KOYAMADA</author-name> (
                <affiliation>Kyoto University, Japan</affiliation>)</author>
              <talk-abstract>In making climate change adaptation policies, the importance of effective methods for extracting and visualizing data for large-scale ensemble climate simulation is increasing. For that purpose, a novel data handling approach for such ensemble
                climate simulation data is necessary. In this study, we present an efficient visualization system for exploring the ensemble climate simulation data. The proposed system provides a data handling method based on a relational database. Users
                can easily retrieve data which are required for research and decision making with the proposed system. We show an application example to demonstrate the effectiveness of the proposed system.</talk-abstract>
            </talk>
            <talk>
              <talk-title>TextTimeline: Visualizing Acoustic Features and Vocalized Timing along Display Text</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Tomoyasu Nakano</author-name> (
                <affiliation>National Institute of Advanced Industrial Science and Technology (AIST), Japan</affiliation>)</author>
              <author>
                <author-name>Jun Kato</author-name> (
                <affiliation>National Institute of Advanced Industrial Science and Technology (AIST), Japan</affiliation>)</author>
              <author>
                <author-name>Masataka Goto</author-name> (
                <affiliation>National Institute of Advanced Industrial Science and Technology (AIST), Japan</affiliation>)</author>
              <talk-abstract>We propose a novel interface, called TextTimeline, that visualizes acoustic features (e.g., intensity), the timing of each word, and each word’s time length (duration) along its displayed text. Although there were interfaces that visualized
                acoustic features of utterances with associated text and displayed those features at the position of the text, the time axis of utterance was nonlinearly stretched according to the width of the displayed text, and the duration information
                was lost. TextTimeline visualizes the duration of each word while maintaining the width of the corresponding text. To visualize the acoustic features of each syllable, in addition to the text that is written horizontally, a vertical (orthogonally
                oriented) axis is added as a sound timeline. This is an interface that makes it possible to visualize the utterance style in detail while giving priority to text display.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Expert Team Mining and Visualization</talk-title>
              <talk-type class="Poster">Poster</talk-type>
              <author>
                <author-name>Yang Wang</author-name> (
                <affiliation>Computer Network Information Center, Chinese Acadamy of Sciences, China</affiliation>)</author>
              <author>
                <author-name>Minzhu Yu</author-name> (
                <affiliation>Computer Network Information Center, Chinese Acadamy of Sciences, China</affiliation>)</author>
              <author>
                <author-name>Guihua Shan</author-name> (
                <affiliation>Computer Network Information Center, Chines Academy of Sciences, China</affiliation>)</author>
              <author>
                <author-name>Yifei An</author-name> (
                <affiliation>Ocean University of China, China</affiliation>)</author>
              <author>
                <author-name>Xiaoxiao Yang</author-name> (
                <affiliation>University of Chinese Academy of Sciences, China</affiliation>)</author>
              <talk-abstract>Expert team mining and visualization is a fundamental process in team competitiveness analysis and an effective tool for investors to choose investee research teams. In this paper, we discover experts in different institutes and research
                fields. Then we propose an algorithm to mine expert teams among co-authors and employ adjacent matrix to visualize co-author relationship and expert teams. Subsequently, we visualize the relationship among expert team members, their papers
                and affiliations. Finally, a visualization system is implemented for expert team analysis.</talk-abstract>
            </talk>
          </p>
          <h3 id="visual-data-storytelling-contest-2">Visual Data Storytelling Contest #2</h3>
          <p>
            <talk>
              <talk-title><a href="https://k-dasu.github.io/">Learning About Disease Associations in Taiwan</a></talk-title>
              <talk-type class="Contest">Contest</talk-type>
            </talk>
          </p>
          <p>
            <author>
              <author-name>Keshav Dasu</author-name> (
              <affiliation>University of California Davis</affiliation>)</author>
            <author>
              <author-name> Suyun Bae</author-name> (
              <affiliation>University of California Davis</affiliation>)</author>
            <author>
              <author-name> Takanori Fujiwara</author-name> (
              <affiliation>University of California Davis</affiliation>)</author>
            <author>
              <author-name> Kwan-Liu Ma</author-name> (
              <affiliation>University of California Davis</affiliation>)</author>
            <contest-vimeo><iframe src="https://player.vimeo.com/video/258174506" width="640" height="337" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe></contest-vimeo>
          </p>
          <p>
            <talk-abstract>We analyzed the data from Taiwan’s National Health Insurance Research Database (NHIRDB). It is stratified into ten age groups, and for each group the likelihood of disease B contracted from disease A is calculated. We depict the disease association
              among the studied subjects by designing a visualization that allows us to answer questions, such as “If I have heart-failure what else am I susceptible to?” and “How does age play a role?” Our visualization is modeled after a virus cell-based.
              This visualization illustrates the co-occurrence of diseases. The nucleus size denotes the number of diseases that are associated. Each flagellum represents another disease category. Longer length of the flagellum represents a stronger association.
              The thickness of a flagellum represents the number of diseases with the same association at a given point. Fluctuation of a flagellum shows variation in the strength of association.</talk-abstract>
          </p>
          <p>
            <talk>
              <talk-title>A Visualization of Two-stage Autoignition of n-dodecane</talk-title>
              <talk-type class="Contest">Contest</talk-type>
            </talk>
          </p>
          <p>
            <author>
              <author-name>Yucong Ye</author-name> (
              <affiliation>University of California Davis</affiliation>)</author>
            <author>
              <author-name>Min Shih</author-name> (
              <affiliation>University of California Davis</affiliation>)</author>
            <author>
              <author-name> Franz Sauer</author-name> (
              <affiliation>University of California Davis</affiliation>)</author>
            <author>
              <author-name> Kwan-Liu Ma</author-name> (
              <affiliation>University of California Davis</affiliation>)</author>
            <author>
              <author-name>Giulio Borghes</author-name> (
              <affiliation>Sandia National Laboratories</affiliation>)</author>
            <author>
              <author-name>Alexander Krisman</author-name> (
              <affiliation>Sandia National Laboratories</affiliation>)</author>
            <author>
              <author-name>Jacqueline Chen</author-name> (
              <affiliation>Sandia National Laboratories</affiliation>)</author>
            <contest-vimeo><iframe src="https://player.vimeo.com/video/258174050" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe></contest-vimeo>
          </p>
          <p>
            <talk-abstract>This animation of ignition in a combustion engine was made through consultations with domain scientists to help develop the story. It uses large multivariate time-varying volumetric data requiring significant effort in data preprocessing,
              volume rendering, and video processing. A dedicated volume rendering pipeline was developed to enable compositing of multiple variables, modulating opacity through distance functions to reduce clutter, and stretching volumes to emphasize
              regions of interest. In addition, the appearance of each variable was carefully designed with scientists to match the impression of its physical properties. Although there are hundreds of timesteps, the frequency is too low for a smooth
              animation, so we utilize optical flow to interpolate between frames in image space. To help provide context, an animation of a diesel engine was made using the Blender 3D software.</talk-abstract>
          </p>
          <p>
            <talk>
              <talk-title><a href="http://perfume-story.herokuapp.com">Perfume Explorer: What Kind of Perfume Do People Prefer? What Is Suitable Perfume for Me?</a></talk-title>
              <talk-type class="Contest">Contest</talk-type>
            </talk>
          </p>
          <p>
            <author>
              <author-name>Dongyeong Lee</author-name> (
              <affiliation>Ajou University</affiliation>)</author>
            <author>
              <author-name>Yeji Jeong</author-name> (
              <affiliation>Ajou University</affiliation>)</author>
            <author>
              <author-name>Hyunsu Park</author-name> (
              <affiliation>Ajou University</affiliation>)</author>
            <author>
              <author-name>Joonsun Hwang</author-name> (
              <affiliation>Ajou University</affiliation>)</author>
            <author>
              <author-name>Kyungwon Lee</author-name> (
              <affiliation>Ajou University</affiliation>)</author>
            <contest-vimeo><iframe src="https://player.vimeo.com/video/258174052" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe></contest-vimeo>
          </p>
          <p>
            <talk-abstract>We tried to express aesthetic elements such as fragrance through data. By collecting subjective evaluation data for over 1000 popular perfumes, we have found a specific pattern. To effectively see at a glance, we visualized a large amount
              of perfume data using ‘Concentric Radviz’. We categorized perfume data into four dimensions: season, weight, persistence, and base material and assigned each to a circle. The coordinates of the node in the concentric circle is determined
              by the spring force, which is assigned to each circular dimension anchor. Based on existing studies, we achieved our initial goal of visualization and then made various attempts to make data classification and exploration easier. From this,
              users will not only be able to find their own perfume, but users will also be able to learn the general characteristics of perfume.</talk-abstract>
          </p>
          <p>
            <talk>
              <talk-title><a href="http://intervis-projects.ccs.neu.edu/refugeescholars-story">The diverging paths of Hilda Geiringer and Leonore Brecher during WWII</a></talk-title>
              <talk-type class="Contest">Contest</talk-type>
            </talk>
          </p>
          <p>
            <author>
              <author-name>Michail Schwab</author-name> (
              <affiliation>Northeastern University</affiliation>)</author>
            <author>
              <author-name>Aditeya Pandey</author-name> (
              <affiliation>Northeastern University</affiliation>)</author>
            <author>
              <author-name>John Wihbey</author-name> (
              <affiliation>Northeastern University</affiliation>)</author>
            <author>
              <author-name>Laurel Leff</author-name> (
              <affiliation>Northeastern University</affiliation>)</author>
            <author>
              <author-name>Michelle Borkin</author-name> (
              <affiliation>Northeastern University</affiliation>)</author>
            <talk-abstract>“Rediscovering the Refugee Scholars” is a research effort by Northeastern University researchers in Jewish Studies, Journalism, Public History, and Computer Science to retrace the forgotten career and life pathways of a group of scholars who
              attempted to flee Nazi persecution. Over 5,000 scholars, including 80 female scientists and mathematicians, sought the help of the Emergency Committee in Aid of Displaced Foreign Scholars based in New York. The committee eventually aided
              scholars throughout Nazi-controlled Europe. With the assistance of the New York Public Library, we have examined, digitized, and analyzed the archival files of the female science and mathematics scholars’ applications to the Emergency Committee,
              and compiled a story about two of their many fates at https://intervis-projects.ccs.neu.edu/refugeescholars-story/. We utilized our novel javascript web-based tool Storytimeline, combining geotemporal data, text and images into a coherent
              narrative visualization. Storytimeline supports author-curated non-linear story telling with a highlighted, multifaceted timeline.</talk-abstract>
          </p>
          <h2 id="Session-4-Time-Oriented-Data">Session 4: Time-Oriented Data</h2>
          <p>
            <session-start>April 12 (Thursday) 14:40</session-start>
            <session-chair>Chair: Fabian Beck (
              <affiliation>University of Stuttgart</affiliation>)</session-chair>
          </p>
          <p>
            <talk>
              <talk-title>Composite Visual Mapping for Time Series Visualization</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Ali Jabbari</author-name> (
                <affiliation>Univ. Grenoble Alpes, LIG, F-38000, France</affiliation>)</author>
              <author>
                <author-name>Renaud Blanch</author-name> (
                <affiliation>Univ. Grenoble Alpes, LIG, France</affiliation>)</author>
              <author>
                <author-name>Sophie Dupuy-Chessa</author-name> (
                <affiliation>Univ. Grenoble Alpes, LIG, F-38000, France</affiliation>)</author>
              <talk-abstract>In the information visualization reference model, visual mapping is the most crucial step in producing a visualization from a data set. The conventional visual mapping maps each data attribute onto a single visual channel (e.g.&nbsp;the
                year of production of a car to the position on the horizontal axis). In this work, we investigate composite visual mapping: mapping single data attributes onto several visual channels, each one representing one aspect of the data attribute
                (e.g.&nbsp;its order of magnitude, or its trend component). We first propose a table which allows us to explore the design space of composite mappings by offering a systematic overview of channel combinations. We expect that using more
                than one visual channel for communicating a data attribute increases the bandwidth of information presentation by displaying separable information on different aspects of data. In order to evaluate this point, we compare horizon graph,
                an existing technique which successfully adopts a composite visual mapping, with a selection of alternative composite mappings. We show that some of those mappings perform as well as –and in some cases even better than– horizon graph in
                terms of accuracy and speed. Our results confirm that the benefits of composite visual mapping are not limited to horizon graph. We thus recommend the use of composite visual mapping when users are simultaneously interested in several
                aspects of data attributes.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Visual Detection of Structural Changes in Time-Varying Graphs Using Persistent Homology</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Mustafa Hajij</author-name> (
                <affiliation>University of South Florida, United States</affiliation>)</author>
              <author>
                <author-name>Bei Wang</author-name> (
                <affiliation>Scientific Computing and Imaging Institute, University of Utah, United States</affiliation>)</author>
              <author>
                <author-name>Carlos Scheidegger</author-name> (
                <affiliation>The University of Arizona, United States</affiliation>)</author>
              <author>
                <author-name>Paul Rosen</author-name> (
                <affiliation>University of South Florida, USA</affiliation>)</author>
              <talk-abstract>Topological data analysis is an emerging area in exploratory data analysis and data mining. Its main tool, persistent homology, has become a popular technique to study the structure of complex, high-dimensional data. In this paper, we propose
                a novel method using persistent homology to quantify structural changes in time-varying graphs. Specifically, we transform each instance of the time-varying graph into a metric space, extract topological features using persistent homology,
                and compare those features over time. We provide a visualization that assists in time-varying graph exploration and helps to identify patterns of behavior within the data. To validate our approach, we conduct several case studies on real-world
                datasets and show how our method can find cyclic patterns, deviations from those patterns, and one-time events in time-varying graphs. We also examine whether a persistence-based similarity measure satisfies a set of well-established,
                desirable properties for graph metrics. </talk-abstract>
            </talk>
            <talk>
              <talk-title>Optimal Sankey Diagrams via Integer Programming</talk-title>
              <talk-type class="Note">Note</talk-type>
              <author>
                <author-name>David Cheng Zarate</author-name> (
                <affiliation>Monash University</affiliation>)</author>
              <author>
                <author-name>Pierre Le Bodic</author-name> (
                <affiliation>Monash University</affiliation>)</author>
              <author>
                <author-name>Tim Dwyer</author-name> (
                <affiliation>Monash University</affiliation>)</author>
              <author>
                <author-name>Graeme Gange</author-name> (
                <affiliation>University of Melbourne</affiliation>)</author>
              <author>
                <author-name>Peter Stuckey</author-name> (
                <affiliation>University of Melbourne</affiliation>)</author>
              <talk-abstract>We present the first practical Integer Linear Programming model for Sankey Diagram layout. We show that this approach is viable in terms of running time for reasonably complex diagrams (e.g.&nbsp;more than 50 nodes and 100 edges) and also
                that the quality of the layout is measurably and visibly better than heuristic approaches in terms of crossing reduction. Finally, we demonstrate that the model is easily extensible (compared to complex heuristics) through the addition
                of constraints, such as arbitrary grouping of nodes.</talk-abstract>
            </talk>
          </p>
          <h2 id="Session-5-Visual-Analytics">Session 5: Visual Analytics</h2>
          <p>
            <session-start>April 12 (Thursday), 16:00</session-start>
            <session-chair>Chair: Jinwook Seo (
              <affiliation>Seoul National University</affiliation>)</session-chair>
          </p>
          <p>
            <talk>
              <talk-title>A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Wenchao Wu</author-name> (
                <affiliation>Siemens Corporate Technology, China</affiliation>)</author>
              <author>
                <author-name>Yixian Zheng</author-name> (
                <affiliation>China Telecom Shanghai Ideal Information Industry (Group) Co.,Ltd, China</affiliation>;
                <affiliation>Shanghai Engineering Research Center of Internet Big Data, Shanghai, China</affiliation>)</author>
              <author>
                <author-name>Kaiyuan Chen</author-name> (
                <affiliation>University of California, Los Angeles, United States</affiliation>)</author>
              <author>
                <author-name>Xiangyu Wang</author-name> (
                <affiliation>University of Southern California, United States</affiliation>)</author>
              <author>
                <author-name>Nan Cao</author-name> (
                <affiliation>Tongji University, China</affiliation>;
                <affiliation>New York University , Shanghai, China</affiliation>)</author>
              <talk-abstract>Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0
                (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line
                equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for
                process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing
                sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced
                analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases
                based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.</talk-abstract>
            </talk>
            <talk>
              <talk-title>GANViz: A Visual Analytics Approach to Understand the Adversarial Game</talk-title>
              <talk-type class="premium">Paper</talk-type>
              <author>
                <author-name>Junpeng Wang</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <author>
                <author-name>Liang Gou</author-name> (
                <affiliation>Visa Research, United States</affiliation>)</author>
              <author>
                <author-name>Hao Yang</author-name> (
                <affiliation>Visa Research, United States</affiliation>)</author>
              <author>
                <author-name>Han-Wei Shen</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <talk-abstract>Generative models bear promising implications to learn data representations in an unsupervised fashion with deep learning. Generative Adversarial Nets (GAN) is one of the most popular frameworks in this arena. Despite the promising results
                from different types of GANs, in-depth understanding on the adversarial training process of the models remains a challenge to domain experts. The complexity and the potential long-time training process of the models make it hard to evaluate,
                interpret, and optimize them. In this work, guided by practical needs from domain experts, we design and develop a visual analytics system, GANViz, aiming to help experts understand the adversarial process of GANs in-depth. Specifically,
                GANViz evaluates the model performance of two subnetworks of GANs, provides evidence and interpretations of the models’ performance, and empowers comparative analysis with the evidence. Through our case studies with two real-world datasets,
                we demonstrate that GANViz can provide useful insight into helping domain experts understand, interpret, evaluate, and potentially improve GAN models.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Visual Analysis of Collective Anomalies Through High-Order Correlation Graph</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Jun Tao</author-name> (
                <affiliation>University of Notre Dame, United States</affiliation>)</author>
              <author>
                <author-name>Lei Shi</author-name> (
                <affiliation>Institute of Software, Chinese Academy of Sciences, Beijing, China, China</affiliation>)</author>
              <author>
                <author-name>Zhou ZHUANG</author-name> (
                <affiliation>Fudan University, China</affiliation>)</author>
              <author>
                <author-name>Congcong Huang</author-name> (
                <affiliation>Chinese Academy of Sciences, China</affiliation>)</author>
              <author>
                <author-name>Rulei Yu</author-name> (
                <affiliation>Institute of Software, Chinese Academy of Sciences, China</affiliation>)</author>
              <author>
                <author-name>Purui Su</author-name> (
                <affiliation>Chinese Academy of Sciences, China</affiliation>)</author>
              <author>
                <author-name>Chaoli Wang</author-name> (
                <affiliation>University of Notre Dame, United States</affiliation>)</author>
              <author>
                <author-name>Yang Chen</author-name> (
                <affiliation>Fudan University, China</affiliation>)</author>
              <talk-abstract>Detecting, analyzing and reasoning collective anomalies is important for many real-life application domains such as facility monitoring, software analysis and security. The main challenges include the overwhelming number of low-risk events
                and their multifaceted relationships which form the collective anomaly, the diversity in various data and anomaly types, and the difficulty to incorporate domain knowledge in the anomaly analysis process. In this paper, we propose a novel
                concept of high-order correlation graph (HOCG). Compared with the previous correlation graph definition, HOCG achieves better user interactivity, computational scalability, and domain generality through synthesizing heterogeneous types
                of nodes, attributes, and multifaceted relationships in a single graph. We design elaborate visual metaphors, interaction models, and the coordinated multiple view based interface to allow users to fully unleash the visual analytics power
                over HOCG. We conduct case studies in two real-life application domains, i.e., facility monitoring and software analysis. The results demonstrate the effectiveness of HOCG in the overview of point anomalies, detection of collective anomalies,
                and reasoning process of root cause analysis.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Visual analytics for networked-guarantee loans risk management</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Zhibin Niu</author-name> (
                <affiliation>School of computer software, Tianjin University, China</affiliation>)</author>
              <author>
                <author-name>Dawei cheng</author-name> (
                <affiliation>Department of Computer Science and Engineering, Shanghai Jiao Tong University,, China</affiliation>)</author>
              <author>
                <author-name>Liqing Zhang</author-name> (
                <affiliation>Department of Computer Science and Engineering, Shanghai Jiao Tong University, China</affiliation>)</author>
              <author>
                <author-name>Jiawan Zhang</author-name> (
                <affiliation>School of Computer Software, Tianjin University, China</affiliation>)</author>
              <talk-abstract>Groups of enterprises can guarantee each other and form complex networks in order to try to obtain loans from banks. Monitoring the financial status of a network, and preventing or reducing systematic risk in case of a crisis, is an area
                of great concern for the regulatory commission and for the banks. We set the ultimate goal of developing a visual analytic approach and tool for risk dissolving and decision-making. We have consolidated four main analysis tasks conducted
                by financial experts: i) Multi-faceted Default Risk Visualization, whereby a hybrid representation is devised to predict the default risk and an interface developed to visualize key indicators; ii) Risk Guarantee Patterns Discovery. We
                follow the Shneiderman mantra guidance for designing interactive visualization applications, whereby an interactive risk guarantee community detection and a motif detection based risk guarantee pattern discovery approach are described;
                iii) Network Evolution and Retrospective, whereby animation is used to help users to understand the guarantee dynamic; iv) Risk Communication Analysis. The temporal diffusion path analysis can be useful for the government and banks to
                monitor the spread of the default status. It also provides insight for taking precautionary measures to prevent and dissolve systematic financial risk. We implement the system with case studies using real-world bank loan data. Two financial
                experts are consulted to endorse the developed tool. To the best of our knowledge, this is the first visual analytics tool developed to explore networked-guarantee loan risks in a systematic manner.</talk-abstract>
            </talk>
            <talk>
              <talk-title>FraudVis: Understanding Unsupervised Fraud Detection Algorithms</talk-title>
              <talk-type class="Note">Note</talk-type>
              <author>
                <author-name>Jiao Sun</author-name> (
                <affiliation>Tsinghua University, Beijing, Beijing, China</affiliation>)</author>
              <author>
                <author-name>Qixin Zhu</author-name> (
                <affiliation>Tsinghua University, Beijing, Beijing, China</affiliation>)</author>
              <author>
                <author-name>Zhifei Liu</author-name> (
                <affiliation>Tsinghua University, Beijing, Beijing, China</affiliation>)</author>
              <author>
                <author-name>Xin Liu</author-name> (
                <affiliation>Tsinghua University, Beijing, Beijing, China</affiliation>)</author>
              <author>
                <author-name>Yueming Wang</author-name> (
                <affiliation>Tsinghua University, Beijing, Beijing, China</affiliation>)</author>
              <author>
                <author-name>Jihae Lee</author-name> (
                <affiliation>Tsinghua University, Beijing, Beijing, China</affiliation>)</author>
              <author>
                <author-name>Shengdong Yang</author-name> (
                <affiliation>Tsinghua University, Beijing, Beijing, China</affiliation>)</author>
              <author>
                <author-name>Lei Shi</author-name> (
                <affiliation>Institute of Software, Chinese Academy of Sciences, Beijing, Beijing, China</affiliation>)</author>
              <author>
                <author-name>Ling Huang</author-name> (
                <affiliation>Tsinghua University, Beijing, Beijing, China</affiliation>)</author>
              <author>
                <author-name>Wei Xu</author-name> (
                <affiliation>Tsinghua University, Beijing, Beijing, China</affiliation>)</author>
              <talk-abstract>Discovering fraud user behaviors is vital to keep online websites healthy. Fraud behaviors usually exhibit grouping behaviors, and researchers have effectively leveraged this behavior to design unsupervised algorithms to detect fraud user
                groups. In this work, we propose a visualization system, namely FraudVis, to visually analyze the unsupervised fraud detection algorithm from temporal, intra-group correlation, inter-group correlation, feature selection, and the individual
                user perspectives. Our system helps domain experts better understand the algorithm output and the detected fraud behaviors. Meanwhile, FraudVis also helps algorithm experts to fine-tune the algorithm design through visual comparison. By
                using the visualization system, we solve two real world cases on fraud detection, one for a social video website and another for an e-commercial website. The result on both cases demonstrate the effectiveness of FraudVis in understanding
                unsupervised fraud detection algorithms.</talk-abstract>
            </talk>
            <talk>
              <talk-title>HeloVis : a Helicoidal Visualization for SIGINT Analysis using 3D immersion</talk-title>
              <talk-type class="Note">Note</talk-type>
              <author>
                <author-name>Alma Cantu</author-name> (
                <affiliation>IMT Atlantique, Brest, France</affiliation>;
                <affiliation>Lab-STICC, Brest, France</affiliation>)</author>
              <author>
                <author-name>Olivier Grisvard</author-name> (
                <affiliation>IMT Atlantique, Brest, France</affiliation>;
                <affiliation>Lab-STICC, Brest, France</affiliation>)</author>
              <author>
                <author-name>Thierry Duval</author-name> (
                <affiliation>IMT Atlantique, Brest, France</affiliation>;
                <affiliation>Lab-STICC, Brest, France</affiliation>)</author>
              <author>
                <author-name>Gilles Coppin</author-name> (
                <affiliation>IMT Atlantique, Brest, France</affiliation>;
                <affiliation>Lab-STICC, Brest, France</affiliation>)</author>
              <talk-abstract>In this paper we present HeloVis: a 3D interactive visualization that relies on immersive properties to improve the user performance during SIGINT analysis. SIGINT, which stands for SIGnal INTelligence, is a field facing many challenges
                like huge amount of data, complex data and novice users. HeloVis draws on perceptive biases, highlighted by Gestalt laws, and on depth perception to enhance the recurrence properties contained into the data and to abstract from interferences
                such as noise or missing data. In this paper, we first present SIGINT and the challenges that it brings to visual analytics. Then, we present the existing work that is currently used or that fits the SIGINT context. Finally, we present
                HeloVis, an innovative application on immersive context that allows performing SIGINT analysis and we present its evaluation performed with military operators who are the targeted end-users of SIGINT analysis.</talk-abstract>
            </talk>
          </p>
          <h2 id="Session-6-Maps-Text-and-Social-Media">Session 6: Maps, Text, and Social Media</h2>
          <p>
            <session-start>April 13 (Friday), 9:00</session-start>
            <session-chair>Chair: Xiaoru Yuan (
              <affiliation>Peking University</affiliation>)</session-chair>
          </p>
          <p>
            <talk>
              <talk-title>Visual Interactive Map Matching</talk-title>
              <talk-type class="premium">Paper</talk-type>
              <author>
                <author-name>Robert Krueger</author-name> (
                <affiliation>Institute for Visualization and Interactive Systems, University of Stuttgart, Germany</affiliation>)</author>
              <author>
                <author-name>Georgi Simeonov</author-name> (
                <affiliation>Institute for Visualization and Interactive Systems, Germany</affiliation>)</author>
              <author>
                <author-name>Fabian Beck</author-name> (
                <affiliation>paluno, University of Duisburg-Essen, Germany</affiliation>)</author>
              <author>
                <author-name>Thomas Ertl</author-name> (
                <affiliation>Institute for Visualization and Interactive Systems (VIS), Germany</affiliation>)</author>
              <talk-abstract>Map matching is the process of assigning observed geographic positions of vehicles and their trajectories to the actual road links in a road network. In this paper, we present Visual Interactive Map Matching, a visual analytics approach
                to fine-tune the data preprocessing and matching process. It is based on ST-matching, a state-of-the-art and easy-to-understand map matching algorithm. Parameters of the preprocessing step and algorithm can be optimized with immediate
                visual feedback. Visualizations show current matching issues and performance metrics on a map and in diagrams. Manual and computer-supported editing of the road network model leads to a refined alignment of trajectories and roads. We demonstrate
                our approach with large-scale taxi trajectory data. We show that optimizing the matching on a subsample results in considerably improved matching quality, also when later scaled to the full dataset. An optimized matching ensures data faithfulness
                and prevents misinterpretation when the matched data might be investigated in follow-up analysis.</talk-abstract>
            </talk>
            <talk>
              <talk-title>MeetingVis: Visual Narratives to Assist in Recalling Meeting Context and Content</talk-title>
              <talk-type class="premium">Paper</talk-type>
              <author>
                <author-name>Yang Shi</author-name> (
                <affiliation>IDVX Lab, College of Design and Innovation, Tongji University, China</affiliation>)</author>
              <author>
                <author-name>Chris Bryan</author-name> (
                <affiliation>University of California-Davis, United States</affiliation>)</author>
              <author>
                <author-name>Sridatt Bhamidipati</author-name> (
                <affiliation>University of California, Davis, United States</affiliation>)</author>
              <author>
                <author-name>Ying Zhao</author-name> (
                <affiliation>Central south university, China</affiliation>)</author>
              <author>
                <author-name>Yaoxue Zhang</author-name> (
                <affiliation>University of Central South University, China</affiliation>)</author>
              <author>
                <author-name>Kwan-Liu Ma</author-name> (
                <affiliation>University of California, Davis, United States</affiliation>)</author>
              <talk-abstract>content from a previously held meeting can lead to better planning and preparation. However, ineffective meeting summaries can impair this process, especially when participants have difficulty remembering what was said and what its context
                was. &nbsp;To assist with this process, we introduce MeetingVis, a narrative-based approach visually summarizing meetings. MeetingVis is composed of two primary components: (1) a data pipeline that processes the spoken audio from a group
                discussion, and (2) a visual-based interface that efficiently displays the summarized content. To design MeetingVis, we create a taxonomy of relevant meeting data points, identifying salient elements to promote recall and reflection. These
                are mapped to an augmented storyline visualization, which combines the display of participant activities, topic evolutions, and task assignments. For evaluation, we conduct a qualitative user study with five groups. Feedback indicates
                that MeetingVis effectively triggers the recall of subtle details from prior meetings: all study participants were able to remember new details, points, and tasks compared to an unaided, memory-only baseline. This visual-based approaches
                can also potentially enhance the productivity of both individuals and the whole team.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Predominance Tag Maps</talk-title>
              <talk-type class="premium">Paper</talk-type>
              <author>
                <author-name>Martin Reckziegel</author-name> (
                <affiliation>Leipzig University, Germany</affiliation>)</author>
              <author>
                <author-name>Gerik Scheuermann</author-name> (
                <affiliation>Leipzig University, Germany</affiliation>)</author>
              <author>
                <author-name>Muhammad Faisal Cheema</author-name> (
                <affiliation>Leipzig University, Germany</affiliation>)</author>
              <author>
                <author-name>Stefan Jaenicke</author-name> (
                <affiliation>Leipzig University, Germany</affiliation>)</author>
              <talk-abstract>A predominance map expresses the predominant data category for each geographical entity and colors are used to differentiate a small number of data categories. &nbsp;In tag maps, many data categories are present in the form of different
                tags, but related tag map approaches do not account for predominance, as tags are either displaced from their respective geographical locations or visual clutter occurs. &nbsp;We propose predominance tag maps, a layout algorithm that accounts
                for predominance for arbitrary aggregation granularities. &nbsp;The algorithm is able to utilize the font sizes of the tags as visual variable and it is further configurable to implement aggregation strategies beyond visualizing predominance.
                &nbsp;We introduce various measures to evaluate numerically the qualitative aspects of tag maps regarding local predominance, global features, and layout stability and we comparatively analyze our method to the tag map approach by Thom
                et al.&nbsp;on the basis of real world data sets. &nbsp;</talk-abstract>
            </talk>
            <talk>
              <talk-title>Visualizing Deep Neural Networks for Text Analytics</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Shaoliang Nie</author-name> (
                <affiliation>CS Department, NC State University, United States</affiliation>)</author>
              <author>
                <author-name>Christopher Healey</author-name> (
                <affiliation>CS Department, NC State University, United States</affiliation>)</author>
              <author>
                <author-name>Kalpesh Padia</author-name> (
                <affiliation>CS Department, NC State University, United States</affiliation>)</author>
              <author>
                <author-name>Sam Leeman-Munk</author-name> (
                <affiliation>SAS Institute Inc, United States</affiliation>)</author>
              <author>
                <author-name>Jordan Benson</author-name> (
                <affiliation>SAS Institute Inc, United States</affiliation>)</author>
              <author>
                <author-name>Dave Caira</author-name> (
                <affiliation>SAS Institute Inc, United States</affiliation>)</author>
              <author>
                <author-name>Saratendu Sethi</author-name> (
                <affiliation>SAS Institute Inc, United States</affiliation>)</author>
              <author>
                <author-name>Ravi Devarajan</author-name> (
                <affiliation>SAS Institute Inc, United States</affiliation>)</author>
              <talk-abstract>Deep neural networks (DNNs) have made tremendous progress in many different areas in recent years. How these networks function internally, however, is often not well understood. Advances in understanding DNNs will benefit and accelerate
                the development of the field. We present TNNVis, a visualization system that supports understanding of deep neural networks specifically designed to analyze text. TNNVis focuses on DNNs composed of fully connected and convolutional layers.
                It integrates visual encodings and interaction techniques chosen specifically for our tasks. The tool allows users to: (1) visually explore DNN models with arbitrary input using a combination of node–link diagrams and matrix representation;
                (2) quickly identify activation values, weights, and feature map patterns within a network; (3) flexibly focus on visual information of interest with threshold, inspection, insight query, and tooltip operations; (4) discover network activation
                and training patterns through animation; and (5) compare differences between internal activation patterns for different inputs to the DNN. These functions allow neural network researchers to examine their DNN models from new perspectives,
                producing insights on how these models function. Clustering and summarization techniques are employed to support large convolutional and fully connected layers. Based on several part of speech models with different structure and size,
                we present multiple use cases where visualization facilitates an understanding of the models.</talk-abstract>
            </talk>
            <talk>
              <talk-title>TagNet: Toward Tag-based Sentiment Analysis of Large Social Media Data</talk-title>
              <talk-type class="Note">Note</talk-type>
              <author>
                <author-name>Yang Chen</author-name> (
                <affiliation>I4Data, Hoboken, New Jersey, United States</affiliation>)</author>
              <talk-abstract>Hashtags and replies, originally introduced on Twitter, have become the most popular ways to tag short messages in social networks. While the primary use of these human-labeled metadata is still for retrieving or clustering messages, there
                have been increasing attempts to use them as subject or topic indicators in measuring people’s continuous sentiments in those messages. However, conducting the analysis of large social media data is still challenging due to the message
                volume, heterogeneity, and temporal dependence. In this paper, we present TagNet, a novel visualization approach tailored to the tag-based sentiment analysis. The approach combines the traditional tag clouds with an improved node-link
                diagram to represent the time-varying heterogeneous information in coherent displays. To help users compare the complex information across different tag groups and subsets of data, the approach leverages a force model to generate layout
                aesthetics and enhance it with visual encodings. Interaction tools are also provided to improve the scalability for exploring large datasets. An example dataset of Twitter posts illustrates the applicability and usefulness of TagNet.</talk-abstract>
            </talk>
          </p>
          <h2 id="Session-7-Evaluation-and-Immersion">Session 7: Evaluation and Immersion</h2>
          <p>
            <session-start>April 13 (Friday), 11:10</session-start>
            <session-chair>Chair: Matthew Brehmer (
              <affiliation>Microsoft Research</affiliation>)</session-chair>
          </p>
          <p>
            <talk>
              <talk-title>An Evaluation of Perceptually Complementary Views for Multivariate Data</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Chunlei Chang</author-name> (
                <affiliation>Monash university , Australia</affiliation>)</author>
              <author>
                <author-name>Tim Dwyer</author-name> (
                <affiliation>Monash University, Australia</affiliation>)</author>
              <author>
                <author-name>Kim Marriott</author-name> (
                <affiliation>Monash University, Australia</affiliation>)</author>
              <talk-abstract>We evaluate the relative merits of three techniques for visualising multivariate data: parallel coordinates; scatterplot matrix; and a side-by-side, coordinated combination of these views. &nbsp;In particular, we report on: &nbsp;(1) the
                most effective visual encoding of multivariate data for each of the six common tasks considered; &nbsp;(2) common strategies that our participants used when the two views were combined based on eye-tracking data analysis; &nbsp;(3) the
                finding that these views are perceptually complementary in the sense that they both show the same information, but with different and complementary support for different types of analysis. &nbsp;For the combined view, our studies show
                that there is a perceptually complementary effect in terms of significantly improved accuracy for certain tasks, but that there is a small cost in terms of slightly longer completion time than the faster of the two techniques alone. Eye-movement
                data shows that for many tasks participants were able to swiftly switch their strategies after trying both in the training phase.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Know Your Enemy: Identifying Quality Problems of Time Series Data</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Theresia Gschwandtner</author-name> (
                <affiliation>Vienna University of Technology, Austria</affiliation>)</author>
              <author>
                <author-name>Oliver Erhart</author-name> (
                <affiliation>Vienna University of Technology, Austria</affiliation>)</author>
              <talk-abstract>Sensible data analysis requires data quality control. An essential part &nbsp;of this is data profiling, which is the identification and assessment of &nbsp;data quality problems as a prerequisite for adequately handling these &nbsp;problems.
                Differentiating between actual quality problems and unusual, &nbsp;but valid data values requires the “human-in-the-loop” through &nbsp;the use of visual analytics. Unfortunately, existing approaches for &nbsp;data profiling do not adequately
                support the special characteristics &nbsp;of time, which is imperative to identify quality problems in time &nbsp;series data – a data type prevalent in a multitude of disciplines. In &nbsp;this design study paper, we outline the design,
                implementation, and &nbsp;evaluation of “Know Your Enemy” (KYE) – a visual analytics approach &nbsp;to assess the quality of time series data. KYE supports the &nbsp;task of data profiling with (1) predefined data quality checks, (2) &nbsp;user-definable,
                customized quality checks, (3) interactive visualization &nbsp;to explore and reason about automatically detected problems, &nbsp;and (4) the visual identification of hidden quality problems.</talk-abstract>
            </talk>
            <talk>
              <talk-title>An Automatic Data Deformation Approach for Occlusion Free Egocentric Data Exploration</talk-title>
              <talk-type class="Paper">Paper</talk-type>
              <author>
                <author-name>Cheng Li</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <author>
                <author-name>Joachim Moortgat</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <author>
                <author-name>Han-Wei Shen</author-name> (
                <affiliation>The Ohio State University, United States</affiliation>)</author>
              <talk-abstract>Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous
                studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages
                the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with
                different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.</talk-abstract>
            </talk>
            <talk>
              <talk-title>Exploring the Role of Sound in Augmenting Visualization to Enhance User Engagement</talk-title>
              <talk-type class="Note">Note</talk-type>
              <author>
                <author-name>Meng Du</author-name> (
                <affiliation>University of California, Davis, Davis, CA, USA</affiliation>)</author>
              <author>
                <author-name>Jia-Kai Chou</author-name> (
                <affiliation>University of California, Davis, Davis, California, United States</affiliation>)</author>
              <author>
                <author-name>Chen Ma</author-name> (
                <affiliation>University of California, Davis, Davis, California, United States</affiliation>)</author>
              <author>
                <author-name>Senthil Chandrasegaran</author-name> (
                <affiliation>Univeristy of California, Davis, California, United States</affiliation>)</author>
              <author>
                <author-name>Kwan-Liu Ma</author-name> (
                <affiliation>University of California, Davis, Davis, California, United States</affiliation>)</author>
              <talk-abstract>Studies on augmenting visualization with sound are typically based on the assumption that sound can be complementary and assist in data analysis tasks. While sound promotes a different sense of engagement than vision, we conjecture that
                by augmenting non-speech audio to a visualization can not only help enhance the users’ perception of the data but also increase their engagement with the data exploration process. We have designed a preliminary user study totest users’
                performance and engagement while exploring in a data visualization system under two different settings: visual-only and audiovisual. For our study, we used basketball player movement data in a game and created an interactive visualization
                system with three linked views. We supplemented sound to the visualization to enhance the users’ understanding of a team’冱 offensive/defensive behavior. The results of our study suggests that we need to better understand the effect of
                sound choice and encoding before considering engagement. We also find that sound can be useful to draw novice users’ attention to patterns or anomalies in the data. Finally, we propose follow-up studies with designs informed by the findings
                from this study.</talk-abstract>
            </talk>
            <talk>
              <talk-title>A comparative 3D visualization tool for observation of mode water</talk-title>
              <talk-type class="Note">Note</talk-type>
              <author>
                <author-name>Midori Yano</author-name> (
                <affiliation>Ochanomizu University</affiliation>)</author>
              <author>
                <author-name>Takayuki Itoh</author-name> (
                <affiliation>Ochanomizu University</affiliation>)</author>
              <author>
                <author-name>Yuusuke Tanaka</author-name> (
                <affiliation>Japan Agency for Marine-Earth Science and Technology</affiliation>)</author>
              <author>
                <author-name>Daisuke Matsuoka</author-name> (
                <affiliation>Japan Agency for Marine-Earth Science and Technology</affiliation>)</author>
              <author>
                <author-name>Fumiaki Araki</author-name> (
                <affiliation>Japan Agency for Marine-Earth Science and Technology</affiliation>)</author>
              <talk-abstract>Mode water forms a 3D region of seawater mass, which has similar physical characteristics values. Research and observation of mode water have a long history in physical oceanography because analysis of mode water brings the understanding
                of various natural phenomena. There have been various definitions of mode water, and comparison of mode water regions extracted with such various definitions is an important issue in this field. This paper presents our study on comparative
                3D visualization tool for the comparison of mode water regions. We extract pairs of outer boundaries of mode water regions as isosurfaces and calculates dissimilarity values between the pairs. The tool visualizes the multi-dimensional
                vectors of the dissimilarity values by Parallel Coordinate Plots (PCP) and provides a user interface to specify particular pairs of mode water regions so that we can comparatively visualize the shapes of the regions. This paper introduces
                our experiment on a comparison of mode water regions between an observation and a simulation datasets using the presented tool.</talk-abstract>
            </talk>
            <talk>
              <talk-title>An Evolutionary Signature for Animated Meshes</talk-title>
              <talk-type class="Note">Note</talk-type>
              <author>
                <author-name>Guoliang Luo</author-name> (
                <affiliation>Jiangxi Normal University</affiliation>)</author>
              <author>
                <author-name>Haopeng Lei</author-name> (
                <affiliation>Jiangxi Normal University</affiliation>)</author>
              <author>
                <author-name>Yugen Yi</author-name> (
                <affiliation>Jiangxi Normal University</affiliation>)</author>
              <author>
                <author-name>Yuhua Li</author-name> (
                <affiliation>Zhengzhou University of Light Industry</affiliation>)</author>
              <author>
                <author-name>Chuhua Xian</author-name> (
                <affiliation>South China University of Technology</affiliation>)</author>
              <talk-abstract>With the rapid growing advancement of animation technologies, 3D animated meshes are becoming one of the major data in the industry such as virtual reality. However, treating the animated mesh data efficiently remains a challenging task
                due to its large scale and limited feature descriptors. In this paper, we present an evolutionary signature for animated meshes based on tempo-spatial segmentation. In specific, we first conduct temporal segmentation to a given animated
                meshes with sub-motions, then apply spatial segmentation within each temporal segment, and intersect spatial segmentation result for over segmentation. Thirdly, we represent the segmentation results into graphs. Finally, we devise an edge
                evolution matrix based on the dynamic behaviour of each edge for the evolutionary signature of the input animated mesh. Our experimental results on similarity measurement by using the proposed signature reflect the effectiveness of our
                method. &nbsp;</talk-abstract>
            </talk>
          </p>
          <h1 id="co-located-event">Co-located Event</h1>
          <ul>
            <li><a href="http://itolab.is.ocha.ac.jp/cj2018/">The 2nd China-Japan Joint Visualization Workshop</a></li>
          </ul>


          <!-- Bootstrap core JavaScript ================================================== -->
          <!-- Placed at the end of the document so the pages load faster -->
          <script src="/pvis2018/assets/js/jquery-3.2.1.min.js"></script>
          <script src="/pvis2018/assets/js/jquery.rwdImageMaps.min.js"></script>

          <script>
            window.jQuery || document.write('<script src="/pvis2018/assets/js/vendor/jquery.min.js"><\/script>')
          </script>
          <script src="/pvis2018/assets/js/pvis2018.js"></script>
          <!-- script src="/pvis2018/assets/bootstrap/js/bootstrap.min.js"></script -->

          <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
          <!-- script src="/pvis2018/assets/js/ie10-viewport-bug-workaround.js"></script -->





        </talk-abstract>
      </talk>
    </section>
  </section>
  <section class="container marketing leag" align="right" level="1">
    <footer>
      <p>© Pacific Visualization Symposium 2018. All Rights Reserved.</p>
    </footer>
  </section>
  <script src="/pvis2018/assets/bootstrap/js/bootstrap.min.js"></script>
</body>

</html>
